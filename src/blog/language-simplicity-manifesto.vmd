---
title: Language Complexity: What it is, When it's Good, and How to Do it Right
author: Evan Ovadia
date: Feb 15, 2022
realm: blog
path: blog/language-simplicity
layout: annotated
namespace: c-blog m-annotated
sponsor: me
---


<ignore>
Too often, we lose sight of our goal.

simplicity isnt just something that makes us feel nice, it helps us help our users.

inherent complexity my butt:

when computers started, we thought we would always have to think about how we handled global memory. then we invented a chunk of global memory called "the stack" and suddenly we could reason much more easily about our own function.

we thought we would need to worry about other processes and what they need to do. it was inherent complexity of working with such complex needs. then processes were invented.

we thought we would need to manage memory forever, and carefully track lifetimes. then garbage collection came out, and for the vast majority of software, this inherent complexity just didnt exist.

inherent complexity is the wrong thing to think about.

our job is to reduce complexity, inherent or otherwise, by usefully employing abstractions.

to me, that's the major difference between programming and software engineering: designing good abstractions.

when someone says "that complexity is unavoidable", it's often a way to justify their own broken abstractions.

inherent complexity is often a lie.

avoid complexity. make good abstractions.

some people hate the word "abstraction". yet these are the same people who celebrate the world's most successful abstractions, like Linux and TCP. No, what they hate is bad abstractions. And duh, everyone hates bad abstractions. But they can't say "bad abstractions are bad".

the only weapon we have to fight complexity is abstraction. and like any weapon, it can be misused. that doesn't mean we should forbid abstractions (borrow checker) or make the most powerful abstractions too easy (implementation inheritance), it means we should look at them with clear eyes and use them in the right places.

avoiding complexity is easy. make your interface as simple as possible, and no simpler (link). make the caller aware of only what he needs to be aware of. this is how we *decouple* our code from other code, and make our programs more flexible, which helps us iterate and explore faster, which helps us help our users better.

software is a living breathing thing. as soon as we sacrifice simplicity, we're setting the program up for long term failure.

Unfortunately, many programmers just pay lip service to simplicity. They say they wan't simplicity, but they aren't willing to sacrifice anything for it, no matter how small.

A programmer might reach for the complexity of async/await over threads, because it saves a tiny bit of memory. Suddenly, they have async annotations spreading virally throughout their code, blasting through abstractions, breaking their APIs, and causing refactoring at all indirect callsites.

Instead of using garbage collection, a programmer might reach for a borrow checker, and suddenly the tiniest of leaves can cause a major refactor in the rest of a program.

The problem is that these things leak through abstractions, and couple the caller to the callee in unnecessary ways. We sacrificed our simplicity for a little bit of performance. And then we patted ourselves on the back and said it was better in all ways that mattered.

Simplicity is important. It helps us move faster. Software is a living breathing thing. As a programmer, *you will always be working on software that needs to change* to help its users. Software that resists change is software that resists helping its users.

Imagine someone that's starting out, building something that they think users will love. Then they launch a beta, and discover that users actually needed something a bit different. Unfortunately, they designed their codebase for correctness, then performance, with simplicity as an afterthought. Everything is coupled so tightly that they need refactors in every corner.

A codebase is constantly growing like that. Later on, that company has launched their product, and the users are filing feature requests like crazy. One person wants to use (feature) their new mapping software to map out all the kids on their school bus route. One person wants to use (feature) it to map out all the historical wildfires to better visualize and predict where the next ones will be. One wants to add a spreadsheet sync so that users can submit to a google forms and then they can go in and approve rows.

Unfortunately, it takes them five times the amount of time, because their codebase is still so coupled. So they have to choose only one feature to work on. They choose the last one. Sucks for those other people. The bus driver still does things by hand. The wildfire person just doesn't predict wildfires.

"But if we add Coq and theorem solvers to guarantee our correctness, then we can eliminate a bunch of edge cases and bring our crash rate down from 4% to 3%!"

But that only helps 1% of users, who are doing weird exploratory things in the edge cases. In a userbase of 1000, that's 10 people. Wouldn't we rather help another 200 people who could use this once we have spreadsheet sync? The answer is pretty obvious.

Imagine someone that's just starting out. It's a private alpha. Users in the real world are waiting for the public release, so it can help them. The vast majority of users have simple data, but there are a few dozen out there who have very complex data which is revealing some bugs. The only way to truly eliminate these classes of bugs is to spend two years adding borrow checking (1%), linear types (another 1%), and a theorem solver like Coq (for the last 2%). They spend that two years doing those things. For two years, the driver drives inefficient routes, the firefighters aren't predicting fires. Why? Because we needed perfection. Perfection hurt the users.

*If your software only works for 99% of users that try it, that's great! You're helping a ton of people. Launch it.*

We always need to focus on how much we're helping how many people. Crashes don't hurt people as much as their fixes sometimes. The obvious answer is a balance.

So how do we balance those?

</ignore>


Tomorrow's languages need to be simpler than the ones we have today. That might sound obvious.


It's actually harder than you'd think because *the way we think about complexity in today's languages is mostly wrong.*


Most people think that: [#riddles]

 * Python is as simple as a language can be.
 * C++ keeps getting more and more complex as time goes on.
 * Rust is complex because it has to be.


All of these statements are mostly incorrect. By the end of this article, you'll see why.


This article aims to answer the fundamental questions of complexity: what even _is_ complexity, when is it good or bad, and what makes a language "too complex"? 


<slice>
#riddles: Notes to self:

"Python is simple as a language can be": wrong because it doesnt surface the inherent complexity of types, and so doesnt reduce that complexity for the user

"C++ keeps getting more and more complex as time goes on": wrong because foreach loops actually reduce complexity even though the language gets more complex.

"Rust is complex because it has to be": wrong because it's situational complexity. it's way more complex in web servers.
</slice>


# Complexity Is Relative

In 2014, our team centralized our app's state into a central StateManager V2 which worked similarly to today's Redux.

"The old solution is too complex! The new solution is simple." we said. We replaced it, bugs decreased, and life was better.


In 2017, new members joined the team, and proposed a new StateManager V3.

"The old solution is too complex! This proposal is simple." they said. So they replaced it, and bugs decreased even further, and life was even better.


Of course, V2 was not "too complex". It only became too complex once we had a simpler alternative.


The same is true with languages. *When we say something is simple or too complex, it is relative.*



# Complexity Is Situational

Some language's complexities seem to arise in certain situations and not others. Functional programming is stellar when transforming data, but in cases with a lot of state, we need to bring in more complex concepts such as monads. We might call this **situational complexity.**


Everything below here should be taken in context. If it's a general purpose language, then measure it against general purposes. If it's a domain specific language, measure it against the specific domain.





# Inherent Complexity

Python is often regarded as a simple language, because it doesn't have static typing.


However, even though the compiler doesn't track types, we often still need do that in our heads. The types exist conceptually, even if the compiler doesn't know about them at compile-time. We humans still need to track types manually. This seems to be **inherent complexity**, complexity that is there even if the language does not track it.


need example

mention it's situational


# Artificial Complexity


need another example

mention its situational again



# Complexity is irrelevant, minimize burden

The only thing that matters is how much the user has to think about.

Typescript adds a complex feature, static typing, to help manage the inherent complexity of types.

It's a more complex situation all around!

However, it makes programming easier for the user, and that's what's important.


*We shouldn't minimize complexity, we should minimize burden.*


Other examples:

 * Vale's regions, Pony's `iso`, and Rust's borrow checker were all added to help protect the user from the inherent complexity in multithreading. We need to think less because of it.
 * In some domains, Rust's borrow checker adds complexity, but reduces the programmer's burden of optimizing their code.



The challenge is that they often come hand in hand. For example:

 * some examples of complexity that pisses me off


So what kinds of complexity help burden, and what kinds of complexity reduce burden? Read on!



# Opt-in Complexity

In C#, one can write an entire program without knowing about the `struct` keyword. But if one wants more performance, they can opt-into more complexity. Perhaps this **opt-in complexity** is a good thing?


Other examples:

 * 


# Ignorable Complexity


In Vale's design, one can write an entire program with just the base building blocks: owning references and non-owning references. If one wants to go the extra mile for more performance, they can use inline objects, regions, allocators, and override gen-checks with an operator. However, these don't change the semantics of the program, just the performance characteristics. A new user can successfully ignore these details and know what the program is doing. This **ignorable complexity** seems pretty good. (Note: These features aren't all done, but useful for discussion.)


Counterexample: C++?


# Remediable Complexity


In Java, a foreach loop uses an iterator under the hood.


# Emergent Complexity


In Rust, the borrow checker is based around one central principle, aliasability-xor-mutability (we can have one mutable reference xor many shared references to an object). This simple rule creates a lot of complexity for real-world programming, and makes us use patterns we might not naturally have opted for. Is the borrow checker simple or complex? Or perhaps both: fundamentally simple, but with high **emergent complexity**.


# Up-Front Complexity

Also in Rust, the borrow checker's complexity hits the programmer all at once when learning the language. This is **up-front complexity**, compared to the more gradual complexity of e.g. C#.


# Compounding Complexity

Also in Rust, the borrow checker can clash with other aspects of the language, such as async/await. Two simple features can cause **compounding complexity.**


The opposite of this is "composable" complexity.


# Composable Complexity

Most languages have complex sub-languages to express generics (e.g. `<T extends IShip>` etc). However, Zig was able to combine one feature (comptime) with another feature (static typing) to be able to add generics with no complex sub-language. Their features **aligned to reduce complexity** compared to other languages, assuming they would have added generics anyway (which is uncertain but beside the point).


# A Proper Abstraction

Proper abstraction will surface inherent complexity, 


# How to handle complexity


To make a language great, one should help with the situation's inherent complexity, and not cause artificial complexity. If it's complexity inherent to programming itself, make a general purpose language. If it's inherent to a specific domain, make a domain-specific language.

Now, let's add some complexity to help with inherent complexity.

Firstly, check if it's up-front complexity. Make it ignorable complexity if possible, or opt-in complexity.

Now, ensure it fits with the rest of the language. Avoid compounding complexity. Prefer composable complexity, try to find a way to make it fit better with the rest of the language.



# Conclusion

Of course, these are lofty ideals, but in real language design we need to add complexity to offer the power our users want. Language design is an art in balancing complexities and the powers they enable in the best way possible.

I'd love to hear your opinions! What do you think complexity is? What are some great examples of languages handling complexity well?





# NOTES

Incorporate feedback and comments from https://www.reddit.com/r/ProgrammingLanguages/comments/vtph4p/what_is_language_complexity_when_is_it_good_and/



Adding complexity help manage inherent complexity to reduce burden.
This would be better if we didnt use "complexity" for both the added complexity and the stuff already inherent.



If you don't consider undefined behavior, C is a big step forward. By adding more features (becoming more complex) it makes software simpler. By taking away irrelevant redundant details, we can focus on what makes one piece of code actually different from another.

By adding some language complexity, we've reduced the user's complexity.

C is a good abstraction.

And fundamentally, that's what a programming language is all about: offering the proper abstractions.

Language design is about balancing abstractions and restrictions to give the user the most power with the least complexity.







## Abstract

This article spells out a very desirable aspect of a programming language: simplicity.

It is easy to say "a language should be simple". In practice, such a simplistic notion rarely helps, because simplicity is fundamentally at odds with other goals (speed, memory safety, etc).

This article describes how to identify and strive for simplicity.

Throughout the article, we'll use Java, C++, and Vale as examples. In follow-up articles, we'll measure Rust, Haskell, and Python as well.



## Incidental Complexity

We should try to minimize incidental complexity.



## Inherent Complexity

If we get rid of all of the incidental complexity, is our thing simple?

Not yet. We still have inherent complexity, complexity that must be present.

However, we can reduce the impact of this inherent complexity, such as via abstraction.


## Learning Curve Complexity

Some complexity is easier to keep in our heads than others. Some is easy to learn, for example types. Some keeps haunting you, such as AxM or immutability. Single ownership is somewhere inbetween.


## Simplicity Measures Mental Resource Usage

Simplicity is how well something fits into the programmer's head.

Here's a view with three components that interact. It tangles them together. All of this logic *must* be present.

But if we separate them into sub-components, we suddenly have less complexity, because for any given task we can safely forget a lot of details.

We no longer have to remember more details and constraints.



GC is the king of simplicity.

Adding constraints, like immutability, or AxM, or UB, increases complexity.



# Why We Want Simplicity

simplicity should be one of the top secondary priorities


1. because youll be hiring newgrads and teaching them, and that will take a lot of time. if the language is difficult, this could cause churn, further shortening the usefulness. [# this is the main factor for why I recommend people make their programs in java, believe it or not.] this was a major problem in google, we couldnt hire c++ developers, so we hired people who we had to teach.
2. because youre trying to get new users. if the complexity turns people off, they likely wont come back. its difficult to recover from a failed launch, because people dont readily give a product a second chance.
3. because not everyone looks at programming as a fascinating puzzle to solve. most programmers are trying to get something done, and want the language to get out of the way so they can do it. two kinds of complexity: initial and inherent. example: c++'s enable_shared_from_this is inherent because you keep running into it.


# How We Lose It

simplicity is something we accidentally give up as we pursue other priorities, such as speed or safety.
a language has a reasonably finite complexity budget.


how do you know if a language is complex? don't ask those who are good at it. theyre very quick to profess how easy it was for them, because theyre very invested in the language. ask those who have explored it and moved on.



# Common Sources of Complexity

## Static Typing

simplicity can be moved elsewhere in time. with dynamic typing, we still have to think about types, just later. except we can let ourselves not think about it for a bit, and lean on the compiler to get us back on track. so we dont have to think about it *as much*.


# Non-Tricks to Reduce Complexity

talk about tactical simplicity vs strategic simplicity. we can sacrifice simplicity at one level for simplicity at another.


# Tricks to Reduce Complexity

## Composability

When we can combine two features to achieve the effects of another, thats more simple.

one must look at the combination power of the basics; the basics are the minimum set of features one needs, to combine to write their program.


## Adding Simplifying Features

adding features can make real life simpler. see c++. an example of opt-in complexity?


## Optimization Hints

Like C#'s struct


some tools:
- hints. if someone knows they can ignore something completely, then the language remains simple.
- fix suggestions; if the compiler can show you how to fix something and why, and its guaranteed to solve that problem (unlike rust lol) then the feature has little negative impact.
- have opt-in advanced features. example: c# structs, vale regions, you should be able to make any program knowing less than 5% of the language. if the language forces newbies into another advanced concept every day, its not as good.
  - vale has only one: single ownership.
- familiarity. innovation is good, but wherever possible, stick to familiar syntax and paradigms and build off of them. where you do diverge, make it easy to realize, or optional.










make it as simple as possible, and no simpler
"If I had more time, I would have written a shorter letter". Simplicity takes effort. It takes time.

a lot of language design is deciding how much complexity is in the language and how much is in user-space.



In vale, I had to keep my eyes open for opportunities to simplify.

i'll use vale as an example because it's famously difficult to have speed and safety without incurring a massive complexity cost.

