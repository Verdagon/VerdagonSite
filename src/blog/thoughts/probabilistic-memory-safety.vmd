---
title: On Probabilistic Memory Safety
author: Evan Ovadia
date: Apr 29, 2022
realm: blog
path: blog/probabilistic-memory-safety
layout: annotated
namespace: c-blog m-annotated
sponsor: me
---


> Regarding the probabilistic behavior of collisions - to me this seems really sketchy to have as a foundational part of a language. What exactly is this doing and what are (were) the consequences of a collision  that made it worthwhile to do that? [# Thank you Blake Anderson for the question!]


A good question!


Recall that whenever we dereference a generational reference, it will do a check to make sure the "remembered generation" matches the "actual generation". [# See [Generational References](https://verdagon.dev/blog/generational-references) for more details.]


In a theoretical alternate language, if a check failure just printed something out to stderr and continued, it would be a memory-unsafe language, because the program proceeds to corrupt memory. As the program continues, these corruptions might build up, and we'd have a lot of weird behavior, like C/C++.


However, in Vale, a check failure will halt and blast away the entire program. So, if your program is still running, you can be pretty confident there were no memory corruptions, meaning Vale is a memory-safe language.


Now, about the probabilities involved. In theory, there is currently a 1/2^64 chance, 1 in 18 billion billion, 0.000000000000000005%, that an unsafe access won't be detected.


To put that in perspective:

 * This is about the chance of a cosmic bit-flip or RAM failure; we take this risk every day anyway.
 * Unsafe memory access can't really stay hidden in your code, because the first time they're run, there's a 1-(1/2^64) chance (99.9999999999999999%) that will be detected, and then likely fixed. It's not like C++ where an unsafe access will probably still work.
 * This isn't exploitable, because the actual generation numbers are randomized every run.


In fact, these factors made it clear that 64 bits is probably too much, so we could even reduce it to 32 bits, which is more reasonable.

<slice new-color="afterword"/>

# Notes

Some notes since writing the above.

There's two ways to look at this:

 * Type 1 probability: A program can run for N months until it fails, it will eventually fail if you leave it running long enough. This is of course *not* how generational references work.
 * Type 2 probability: Your airbags deploy when you run your car into a wall. A driver would have to crash on average 2 billion times on average before the airbags fail. Of course, we take their license away (fix the bug) long before that happens.

Type 1 means that a correct program will eventually fail. That's not the case with type 2. Type 2 is an aggressive detection mechanism that will never affect correct operation.


More notes:

 * Generational references work kind of like passwords. If you've ever used a password, generational references will feel familiar.
 * Assuming 32 bit generations, we get a false negative after 3 billion use-after-frees on average. On Earth, asan caught me doing a use-after-free about once a month, so average time to unsafety is about 250 million years. I'd probably have fixed the bug by then.
 * Unsafety from RAM failure is much more likely than unsafety from a generation check false negative.
    * Your average (2012) Dell computer with 4 GB DRAM will have [3 to 240 errors per month](https://youtu.be/aT7mnSstKGs?t=1103).
    * DRAM failure is [25k-70k per billion device hours per mbit](http://www.cs.toronto.edu/~bianca/papers/sigmetrics09.pdf). 8% of DIMMs fail per year.
    * Mitigations: ECC could help with this, but it's generally not offered in consumer devices, probably due to cost/benefit reasons. This is [mitigated](https://superuser.com/a/1635269) in safety-critical use cases like avionics.
 * Still, they're not comparable, as RAM failures are silent, and happen to correct code. Generation check failures are very loud, anda only happen in incorrect code.

</slice>
