---
title: The Impossible Optimization, and the Metaprogramming To Achieve It
author: Evan Ovadia
date: October 27, 2025
realm: blog
path: blog/impossible-optimization
layout: annotated
namespace: c-blog m-annotated
---


<ignore>
TODO:
* scala
* strengthen i am me
* mention that we could theoretically get information about the host machine
* flip perspective: why are langs forcing to runtime? not very zero cost
* parser combinator?
* "if you have a sequence describing things to do"

</ignore>


Every once in a while, you come across an optimization that is so mind-blowingly _weird_, that you feel like you're peering into some sort of alternate dimension.


Until recently, I believed that metaprogramming is just a way to write code that writes code. That's not interesting, we've been doing that for decades! Heck, that's _all you do_ in PHP. And that's C++ templates. Metaprogramming is just C++ templates, plus all the neat compile-time features like `constexpr`, right?


If you believe that like I did, buckle up! I'm going to show you how metaprogramming can be used to unlock what I call *the impossible optimization*, a way to speed up your code tenfold.


! Big shout-out to [Zig](https://ziglang.org/) and Alexandros Naskos for his [ctregex](https://github.com/alexnask/ctregex.zig/tree/master) Zig library. While I was investigating Mojo's dependent types superpowers, I found that library which inspired this whole thing. This post explains that technique, and expands on it to talk about how far we can take it.


## The Impossible Optimization

Imagine that you had a program using a simple regex function, and you wanted to use it to verify emails: [# ...which is a TERRIBLE idea and you should never do it (see [this page](https://stackoverflow.com/questions/201323/how-can-i-validate-an-email-address-using-a-regular-expression)) but what's life without a little regex-related sin!]

```rs
fn main():
    email_regex =
        Regex("\\w+(\\+\\w*)?@(\\d+\\.\\d+\\.\\d+\\.\\d+|\\w+\\.\\w+)")

    print(matches(email_regex, "user@example.com"))      # Prints True
    print(matches(email_regex, "uexample.com"))          # Prints False
    print(matches(email_regex, "user@ecom"))             # Prints False
    print(matches(email_regex, "user+tag@example.com"))  # Prints True
    print(matches(email_regex, "user@100"))              # Prints False
    print(matches(email_regex, "howdy123@1.2.3.4"))      # Prints True
    print(matches(email_regex, "howdy1231.2.3.4"))       # Prints False
    print(matches(email_regex, "howdy123@1/2/3/4"))      # Prints False
```


We all know that regex is pretty slow, because it basically has to run a whole interpreter. Or, if we "precompile" a Regex, it would be interpreting an AST, which is still pretty slow. [#precompileregex]


When you really need speed, you skip regex entirely and make a hand-written `matches` function like this:

```rs
fn matches(text: String) -> Bool:
    var total_consumed = 0
    var pos = 0
    var text_len = len(text)
    var word_matches = 0
    var word_total_consumed = 0
    while True:
        if pos >= text_len:
            break
        var ch = text[pos]
        alias char_class = "word"
        var char_matches = False
        char_matches = (
            (ord("a") <= ord(ch) <= ord("z"))
            or (ord("A") <= ord(ch) <= ord("Z"))
            or (ord("0") <= ord(ch) <= ord("9"))
            or ch == "_"
        )
        if not char_matches:
            break
        var chars_consumed = 1
        if chars_consumed == 0:
            break
        word_matches += 1
        word_total_consumed += chars_consumed
        pos += chars_consumed
    if word_matches < 1:
        return False
    total_consumed += word_total_consumed
    ...  # much more code, for the rest of the pattern
```


This is literally 10x faster.


It makes you wonder, can an optimizer somehow do this for us? Unfortunately, no. There are a lot of challenges in the way, as I'll explain below.


<slice>
#precompileregex: TODO: more explanation here, link out
</slice>


## The Challenge


The problem is that `matches` is kind of like an interpreter. It takes in a constant tree of nodes, like this:

 * SequenceNode for `\w+(\+\w*)?@(\d+\.\d+\.\d+\.\d+|\w+\.\w+)`
    * RepeatNode for `\w+`
       * CharClassNode for `\w`
    * RepeatNode for `(\+\w*)?`
       * SequenceNode for `\+\w*`
          * LiteralNode for `\+`
          * RepeatNode for `\w*`
             * CharClassNode for `\w`
    * LiteralNode for `@`
    * OrNode for `(\d+\.\d+\.\d+\.\d+|\w+\.\w+)`
       * SequenceNode for `\d+\.\d+\.\d+\.\d+`
          * RepeatNode for `\d+`
             * CharClassNode for `\d`
          * LiteralNode for `\.`
          * RepeatNode for `\d+`
             * CharClassNode for `\d`
          * LiteralNode for `\.`
          * RepeatNode for `\d+`
             * CharClassNode for `\d`
          * LiteralNode for `\.`
          * RepeatNode for `\d+`
             * CharClassNode for `\d`
       * SequenceNode for `\w+\.\w+`
          * RepeatNode for `\w+`
             * CharClassNode for `\w`
          * LiteralNode for `\.`
          * RepeatNode for `\w+`
             * CharClassNode for `\w`

...and it also takes in the user's string which is only known at run-time.


It then navigates up and down the tree according to what character is next in the user's string.


! Actual regex implementations are a _lot_ more sophisticated, and use state machines instead of an AST like this. This is my own simplistic Regex subset implementation made just for this post. [# I had to hold myself back from adding even more interesting features to it, like SIMD.]


To illustrate, here's the top-level `matches` function:

```rs

@no_inline
fn matches(regex: Regex, text: String) -> Bool:
    var result = _match_node(regex.nodes, regex.root_idx, text, 0)
    return result.matched and result.chars_consumed == len(text)
```


...which really just calls into the (recursive) `_match_node` function:

```rs
fn _match_node(
    nodes: List[RegexNode], node_idx: Int, text: String, start_pos: Int
) -> MatchResult:
    var node = nodes[node_idx]
    if node.isa[LiteralNode]():
        ref literal_node = node[LiteralNode]
        return _match_literal(nodes, literal_node, text, start_pos)
    elif node.isa[RepeatNode]():
        ref repeat_node = node[RepeatNode]
        return _match_repeat(nodes, repeat_node, text, start_pos)
    elif ...
```

...which calls various functions, like the below `_match_repeat` function. This function checks the repeating parts of the regular expression, like how how email regex's `\w+` uses `+` to repeat the `\w`.

```rs
fn _match_repeat(
    nodes: List[RegexNode],  # All the regex nodes
    node: LiteralNode,       # Which node we're currently on
    text: String,            # User's string
    start_pos: Int           # Where in the user's string we're at
) -> MatchResult:
    var matches = 0
    var total_consumed = 0
    var pos = start_pos
    while True:
        if node.maximum_times >= 0 and matches >= node.maximum_times:
            break
        var result = _match_node(nodes, node.repeated, text, pos)
        if not result.matched:
            break
        if result.chars_consumed == 0:
            break
        matches += 1
        total_consumed += result.chars_consumed
        pos += result.chars_consumed
    if matches >= node.minimum_times:
        return MatchResult(True, total_consumed)
    else:
        return MatchResult(False, 0)
```


One benefit of the recursive approach is that it's very flexible. It can execute an arbitrary tree of nodes; it can execute an arbitrary regular expression. The hand-written version doesn't have that flexibility, it's coded up-front.


But of course, the *flexibility has a cost*: recursion requires more function calls than the hand-written version, and *function calls have overhead.* Every time we call a function, that's pushing and popping on the stack. The hand-written version is faster because it doesn't have that problem.


Another problem is that this general regex code has extra flexibility that the hand-written one doesn't need. Look at this condition from above:

```
    if node.maximum_times >= 0 and matches >= node.maximum_times:
        break
```

This code is only relevant when we want a maximum number of matches. But `\w+` has no upper limit, we want it to match any number of `\w` characters. So it's unfortunate that this useless code is in here. The hand-written version is faster because it doesn't have that problem.


The hand-written version is also theoretically more optimizable because everything is in one function, as if we *inlined all these recursive `_match_node`, `_match_repeat`, etc calls.* Optimizers generally do better when everything is in one function.


Is there a way to get these benefits for our general `Regex` code?


## Can we inline recursive calls?

Above, I said that the hand-written function is almost as if we inlined all these recursive `_match_node` calls.


So the obvious question: can we inline `_match_node`?


Let's try it, by throwing a `@always_inline` on it:

```rs
@always_inline
fn _match_node(
    nodes: List[RegexNode], node_idx: Int, text: String, start_pos: Int
) -> MatchResult:
    ...
```


But this doesn't work of course, because you can't inline a recursive call! No language can, that I know of. [# Unless you count tail-call optimization. And I bet there's a recursion-equivalent of loop unrolling too.]


So let's table this line of thinking for now. We'll come back to it later.


## The Sorcery

I promised something mind-blowingly _weird_, so let's make things weird.


We're going do something I call _The Three Steps of Futamura Sorcery_.


We'll:

 * Parse the regular expression and create the Regex *at compile-time.*
 * Make it a compile-time parameter, and use compile-time operations.
 * Inline.

...and then something amazing will happen.


This won't make sense at first, but read on and I'll break it down step by step.


### Step 1: Compute the Regex at compile-time

To compute the Regex at compile time, we simply need to change the `var regex` to `alias regex`.

Before:

```
# Parse the regex (at runtime)
var regex = Regex("\w+(\+\w*)?@(\d+\.\d+\.\d+\.\d+|\w+\.\w+)")
regex.match(regex.nodes, regex.root_node, user_string, 0)
```

After:

```
# Parse the regex (at compile time)
alias regex = Regex("\w+(\+\w*)?@(\d+\.\d+\.\d+\.\d+|\w+\.\w+)")
_match_node[regex.nodes, regex.root_node](user_string, 0)
```

`alias` is Mojo-speak for "compile-time variable". [# I kind of wish Mojo had a `comptime` keyword like Zig, but alas!]


"But wait Evan, isn't that moving a lot of calculation to compile-time? Isn't that impossible?"

Indeed! The `Regex` constructor is pretty heavy. It's a whole parser, with its own recursion, and heap allocation, and all sorts of stuff. And now it's happening at compile-time.


"Wait, did you say heap allocation? Isn't heap allocation at compile-time impossible?"

Not anymore! Mojo can do it, Zig can do it, [even C++20 can do it](https://accu.org/journals/overload/31/176/fertig/#:~:text=Or%20in,appealing%20use-cases).


### Step 2: Make Regex a compile-time parameter, use compile-time operations


So how do we read this compile-time `Regex` at run-time?

Do we put it into a global or something?

No! We're going to do something weirder: we're going to *only read it at compile-time.*


Before:

```rs
fn _match_node(
    nodes: List[RegexNode],
    node_idx: Int,
    text: String,
    start_pos: Int
) -> MatchResult:
    var node = nodes[node_idx]
    if node.isa[LiteralNode]():
        ref literal_node = node[LiteralNode]
        return _match_literal(nodes, literal_node, text, start_pos)
    elif node.isa[RepeatNode]():
        ref repeat_node = node[RepeatNode]
        return _match_repeat(nodes, repeat_node, text, start_pos)
    elif ...
```


After:

```rs
fn _match_node[nodes: List[RegexNode], node_idx: Int](
    text: String,
    start_pos: Int
) -> MatchResult:
    alias node = nodes[node_idx]
    @parameter
    if node.isa[LiteralNode]():
        alias literal_node = node[LiteralNode]
        return _match_literal[nodes, literal_node](text, start_pos)
    elif node.isa[RepeatNode]():
        alias repeat_node = node[RepeatNode]
        return _match_repeat[nodes, repeat_node](text, start_pos)
    elif ...
```


There are two main changes:

 * The `nodes` and `node_idx` run-time parameters are now *compile-time parameters*, because they're inside `[...]`. We're using Mojo's generics system to hold these values at compile-time.
 * The `@parameter if`. `@parameter` means "at compile time" in Mojo-speak, so this `if` is run at compile-time, not run-time. This is like a `constexpr if` in C++ terms, or in C terms it's more like `#define` than `if`.


That means `_match_node` is now a generic function, a template.


There's only one `nodes` value (from the `Regex`), but there are 26 different `node_idx`s for this `Regex`. That means we'll have 26 different instantiations of this `_match_node`, one for each `node_idx`.


Previously, we had a very simple call tree, with only one `_match_node`, that was recursive. The call tree looked like this:

 * `main`
    * `match`
       * `_match_node`
          * `_match_node`
             * `_match_node`
                * `_match_node`
                   * `_match_node`
                      * ... and so on


Now, we have 26 versions of `_match_node`, with a call tree that looks like this:

 * `main`
    * `match[Regex instance]`
       * `_match_node` for root node
          * `_match_node` for first `\w+`
             * `_match_node` for first `\w`
          * `_match_node` for `(\+\w*)?`
             * `_match_node` for `\+\w*`
                * `_match_node` for `\+`
                * `_match_node` for `\w*`
                   * `_match_node` for `\w`
          * `_match_node` for `@`
          * `_match_node` for `(\d+\.\d+\.\d+\.\d+|\w+\.\w+)`
             * `_match_node` for `\d+\.\d+\.\d+\.\d+`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
             * `_match_node` for `\w+\.\w+`
                * `_match_node` for `\w+`
                   * `_match_node` for `\w`
                * `_match_node` for `\.`
                * `_match_node` for `\w+`
                   * `_match_node` for `\w`


Let's look at the fourth item in that list: the `_match_node` for the first `RepeatNode`.


When the compiler instantiates it as `_match_node[nodes, 5]`, it would look like below. The `@parameter if` is evaluated at compile time, so only one of its branches is included. I'll leave them in as comments to illustrate:

```rs
fn _match_node[nodes, 5](  # Node 5 is a RepeatNode
    text: String,
    start_pos: Int
) -> MatchResult:
    var node = nodes[node_idx]
#   @parameter
#   if node.isa[LiteralNode]():
#       alias literal_node = node[LiteralNode]
#       return _match_literal[nodes, literal_node](text, start_pos)
#   elif node.isa[RepeatNode]():
        alias repeat_node = node[RepeatNode]
        return _match_repeat[nodes, repeat_node](text, start_pos)
#   elif ...
```


Here it is with the comments removed:

```rs
fn _match_node[nodes, 5](  # Node 5 is a RepeatNode
    text: String,
    start_pos: Int
) -> MatchResult:
    alias node = nodes[node_idx]
    alias repeat_node = node[RepeatNode]
    return _match_repeat[nodes, repeat_node](text, start_pos)
```


The benefit here is that we don't have to do the `node.isa[LiteralNode]`, `node.isa[RepeatNode]`, etc. at run-time. That's pretty nice, it eliminates a bit of overhead.


We'll see the biggest benefit in the next section though.


### Step 3: Inline

Let's look at the call tree again:

 * `main`
    * `match[Regex instance]`
       * `_match_node` for `root node`
          * `_match_node` for first `\w+`
             * `_match_node` for first `\w`
          * `_match_node` for `(\+\w*)?`
             * `_match_node` for `\+\w*`
                * `_match_node` for `\+`
                * `_match_node` for `\w*`
                   * `_match_node` for `\w`
          * `_match_node` for `@`
          * `_match_node` for `(\d+\.\d+\.\d+\.\d+|\w+\.\w+)`
             * `_match_node` for `\d+\.\d+\.\d+\.\d+`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
                * `_match_node` for `\.`
                * `_match_node` for `\d+`
                   * `_match_node` for `\d`
             * `_match_node` for `\w+\.\w+`
                * `_match_node` for `\w+`
                   * `_match_node` for `\w`
                * `_match_node` for `\.`
                * `_match_node` for `\w+`
                   * `_match_node` for `\w`


Note how these are *not actually recursive calls* anymore. The compiler doesn't see these as recursive, it sees them as completely different functions.

As far as the compiler is concerned, this is a normal call tree, not a potentially infinite call tree.


So... maybe we can inline them? Can we put `@always_inline` on these functions?

Yes we can! Let's inline _everything_.


Remember this function?

```rs
fn _match_node[nodes, 5](  # Node 5 is a RepeatNode
    text: String,
    start_pos: Int
) -> MatchResult:
    alias node = nodes[node_idx]
    alias repeat_node = node[RepeatNode]
    # Let's inline this _match_repeat call first!
    return _match_repeat[nodes, repeat_node](text, start_pos)
```


Let's inline the `_match_repeat` call first. We do that by putting `@always_inline` on `fn _match_repeat`:

```rs
@always_inline
fn _match_repeat[
    nodes: List[RegexNode], repeat_node: RepeatNode  # Compile-time parameter!
](text: String, start_pos: Int) -> MatchResult:
    var matches = 0
    var total_consumed = 0
    while True:
        ...
```


Now it will be inlined into our `_match_node[nodes, 5]` call, which now looks like this:

```rs
fn _match_node[nodes, 5](  # Node 5 is a RepeatNode
    text: String,
    start_pos: Int
) -> MatchResult:
    alias node = nodes[node_idx]
    alias repeat_node = node[RepeatNode]
    # Inlined `_match_repeat` call
    var matches = 0
    var total_consumed = 0
    while True:
        ...
    ...
```


And let's throw `@always_inline` on all the other functions too:

 * `_match_literal`
 * `_match_charclass`
 * `_match_or`
 * `_match_sequence`
 * `_match_repeat`
 * `_match_node`


### The Result

Suddenly, our large call tree above becomes simply this:

 * `main`, which calls:
    * `match[Regex instance]`


And looking at the generated IR, *it was all folded down into one function*, structured similarly to the hand-written version.


The final results (tested on an M2 Macbook Pro), run 200,000,000 times:

 * Normal recursive version (main_rt.mojo): 135.23 seconds
 * Metaprogrammed version (main_ct.mojo): 13.38 seconds
 * Hand-written version (12.72 seconds)


The metaprogrammed version is *10x faster than the normal recursive version,* and within ~5% of the hand-written version.


## The Impossible Optimization

...isn't so impossible anymore!


It turns out, if we use comptime, recursion, and inlining in the right way, we can accomplish amazing things.


This is "metaprogramming". It's a term I've heard for many, many years, but I didn't truly understand its power and implications until now.


## The Downside

Let's talk about the main downside: *compile times!*


This takes much more time to compile, because the compiler is generating _much_ more code. In this example, we made 26 `_match_node[...]` functions, each about a fifth of the size of the original run-time recursive `_match_node`. So this technique generated about 5x as much `_match_node` code.


One should use this technique wisely, probably only code that needs to run fast.


## The Implications

This regex example is just something most of us can relate to, but it can be used for much more than compiling DSLs. This technique can be pushed ridiculously far.


You could compile entire database transactions, fuse graphics shaders or GPU kernels together, and do all sorts of things with this technique.


*Any time you are making a predictable decision based on constant data, you could apply this technique.*


It's particularly nice because it makes the call tree known at compile-time, which enables inlining, which enables all sorts of optimizations.


Now, some wild speculation. I think you could also use this:

 * To enable more intricate transformations, like in-memory journaled/persistent data structures.
 * With the `MaybeComptime` concept [here](https://github.com/modular/modular/issues/4887#issuecomment-3045841971) (for working with data when you don't know if it's known at compile time) to gracefully fallback to run-time operations.
 * To enable better remote execution of code.


## Prior Art

This technique is known as the "First Futamura Projection", but it's been impossible to do this with mainstream languages until recently.


There are probably a lot of smaller languages can do this, but the biggest three I know of are C++ (as of C++20), Zig, and Mojo.


This entire post was inspired by [Alexandros Naskos](https://github.com/alexnask)'s [ctregex](https://github.com/alexnask/ctregex.zig/tree/master) Zig library, and Zig's [print](https://github.com/ziglang/zig/blob/feb05a716d1ae105cf0e8f9966b6ade7f32dc680/lib/std/Io/Writer.zig#L594) function parses its format string at compile time.


There are likely countless academic papers on this. If you zoom out three levels and squint, one could think of this as just the natural conclusion of good constant propagation.


## Conclusion

(TODO: conclusion here)

If you want to see the code, you can find it [here](https://github.com/Verdagon/MojoCompileTimeRegex).
