---
title: Vale's Memory Safety Strategy: Generational References and Regions
author: Evan Ovadia
date: Updated Apr 10 2023
realm: blog
path: blog/generational-references-v2
layout: annotated
namespace: c-blog m-annotated
sponsor: me
---


TODO:
- link out to an article on linear style
- suffix v1, and make this generational-references.vmd
- get better banchmarks, add them here
- add the caveats from the notes section
- once we have a bigger benchmark with a little more random access, perhaps BenchmarkRL, try to get it to show how little it matters in a random-access application, then explain that the CPU is really good at doing bounds checking and gen checking in parallel
- add to v1: ! This is an out-of-date version of the article [Vale's Memory Safety Strategy: Generational References and Regions](/blog/generational-references).
- sure, we lose a little bit of reproducibility for UAFs, just like rust loses some with its unsafe problems. but overall, vale is actually the best with reproducibility because of deterministic replayability.



*Generational references* are a new memory management technique that's easy, deterministic, and fast. [# This is an updated version of the 2021 article, which you can still find [here](/blog/generational-references-v1)]


Vale also combines generational references with [regions](/blog/zero-cost-borrowing-regions-overview) to enable [zero-cost borrowing](/blog/zero-cost-borrowing-regions-part-1-immutable-borrowing) for when we want to squeeze every last bit of performance out of our programs. [# Regions have been prototyped, but aren't complete yet. See [here](/blog/regions-first-prototype) for more!]


This article explains:

 * How generational references work.
 * How regions make them even faster.
 * How they work together to form Vale's memory safety approach.


# Every object has a single owner

Recall that in Vale, an object is freed when its *owning reference* goes out of scope. An object always has exactly one owning reference pointing to it.

We can have as many *non-owning* references as we want. [# This distinction is similar to C++'s `unique_ptr<T>` and `T*`.]


In other languages, when a programmer frees an object and then accidentally dereferences a non-owning reference to it, it can cause memory unsafety and vulnerabilities.

Generational references detect this situation and react to it safely, [# Such as by halting or stack unwinding.] in a way that doesn't incur extra complexity for the programmer.


# The Simple Version


Here's a simplified explanation of generational references. The actual design is a bit more interesting, but this should be a useful stepping stone.


In C, whenever we allocate a struct on the heap, `malloc` remembers that that location is "in use". Until we `free` it, nobody else can allocate something there.


<<<<
You could imagine that it internally has a bunch of `Allocation` structs, like this. [# The actual `malloc` uses a much faster and more interesting system than the one described here.]
////
```
struct Allocation {
  bool currentlyInUse;
  char allocationBytes[];
};
```
>>>>


By making a slight adjustment here, we can create a system that helps our programs' memory safety.

<<<<
Let's add a `uint64_t currentGeneration;` to it. [# This is similar to the "generational indices" technique from C++ and Rust, but applied to the entire world instead of just a specific vector.]

It starts at zero, and we increment it every time we allocate something to this spot in memory. 
////
```
struct Allocation {
  bool currentlyInUse;
  uint64_t currentGeneration;
  char allocationBytes[];
};
```
>>>>


Now, instead of `malloc` returning a `void*`, let's make a `gmalloc` that returns a "generational reference".

<<<<
A *generational reference* is simply a pointer with a "remembered generation" number next to it.


Our codebase will use `gmalloc` instead of `malloc`, and use generational references instead of raw pointers.


`gmalloc` increments `currentGeneration` every time we allocate anything, and we would also have a `gfree` that increments it whenever we free something.
////
```
struct GenerationalReference {
  void* allocation;
  uint64_t rememberedGeneration;
};

GenerationalReference gmalloc(int size) {
  // Find an Allocation not currentlyInUse.
  // Increment its currentGeneration.
  // Return it.
}
```
>>>>


Now, to make our program memory-safe, we always follow the rule: *always call `__check` before dereferencing a generational reference.*


<<<<
Before:
```
struct Spaceship {
  int numWings;
};

int main() {
  Spaceship* ship =
      (Spaceship*)malloc(sizeof(Spaceship));

  // Set ship's numWings to 2
  ship->numWings = 2;
}
```
////
After:

```
struct Spaceship {
  int health;
  int numWings;
};

int main() {
  GenerationalReference shipRef =
      gmalloc(sizeof(Spaceship));

  // Set ship's numWings to 2
  __check(shipRef);
  ((Ship*)shipRef.allocation)->numWings = 2;
}
```
>>>>


That `__check` function looks at the 8 bytes above the object (at `Allocation`'s `currentGeneration`), and make sure that it matches the generational reference's `rememberedGeneration`. It could look something like this:

```
void __check(GenerationalReference genRef) {
  uint64_t currentGeneration = *(uint64_t*)((char*)genRef.allocation - 8);
  assert(genRef.rememberedGeneration == currentGeneration);
}
```


It's as we're saying:

> *"Hello! I'm looking for the 11th inhabitant of this house, are they still around?"*


and the person who opens the door says:

> *"No, sorry, I'm the 12th inhabitant of this house, the 11th inhabitant is no more."*

or instead:

> *"Yes! That is me. Feel free to dereference!"*



*If we always call `__check` before dereferencing, then our code will be memory-safe.*


Note that so far, all these structs are on the heap. Further below we'll talk about how we can put data on the stack. But first, let's talk about speed!


# Generational References are Fast


The very first versions of Vale used this approach for its memory safety. We benchmarked it, and it outperformed reference counting pretty nicely.


(benchmarks here)


note that vale uses a slightly different approach, which has more overhead in some cases and less in others.


Of course, beating reference counting is a pretty low bar. [# Unless you're a language like Lobster or Nim, which do some really ingenious things with reference counting.] Vale aims to be on par with languages like C++ and Rust. How do we get there?


The picture gets even better: we can skip the vast majority of `__check`s using static analysis, single ownership, and regions.


<slice/>


# Skipping checks with single ownership

Single ownership is a C++ concept which roughly means that "Every object is *owned* by exactly one other object (or local variable). When that owner is destroyed, the owned object is automatically `free`d."


<<<<
In Vale, any object on the stack is owned by the containing stack frame.


When a function accesses data owned by one of its variables, it doesn't need a `__check`.
////
```
struct Spaceship { fuel int; }
exported func main() {
  ship Spaceship = Spaceship(7);

  // Doesn't need a __check!
  println(ship_ref.fuel);
}
```
>>>>


<<<<
Any struct on the heap is owned by an "owning pointer", such as the one in this local variable `ship`.


Same as before, if a function accesses data owned by one of its variables, it doesn't need a `__check`.
////
```
struct Spaceship { fuel int; }
exported func main() {
  ship ^Spaceship = ^Spaceship(7);
}
```
>>>>


If want to access something you don't own, you need a *reference* to it, written `&Spaceship`. Vale's references are all generational references.


<<<<
Vale automatically inserts `__check`s when you access anything you don't own.

(...at least, until the next section when we talk about regions.)
////
```
func PrintFuel(ship_ref &Spaceship) {
  // implicit __check(ship_ref)
  println(ship_ref.fuel);
}
```
>>>>


# Skipping even more checks with regions

We can use [regions](/blog/zero-cost-borrowing-regions-overview)' to safely skip generation checks for a certain scope.


<<<<
We can add `pure` to the above `PrintFuel` function to inform the compiler that we won't change any data that existed before the function call.


In exchange for that, the compiler is able to skip `__check`s for any arguments, or any data those arguments might own.
////
```
pure func PrintFuel(ship_ref &Spaceship) {
  // No __check needed!
  println(ship_ref.fuel);
}
```
>>>>



This is called [zero-cost borrowing](/blog/zero-cost-borrowing-regions-part-1-immutable-borrowing). It's a pretty in-depth topic, check out [this series](/blog/zero-cost-borrowing-regions-overview) which talks about other ways that regions help eliminate `__check`s.


# How many `__check`s remain?


Even if your program does nothing special, the compiler's static analysis can identify pure blocks and other areas where it can eliminate `__check`s, and you still benefit from how libraries use regions under the hood. When our sample program BenchmarkRL uses no regions, it still skips an estimated NUMBER% of all generation checks. [# See LINK HERE blog post for how we get this estimated percentage.]


We can very easily eliminate most `__check`s by just adding `pure` annotations to functions that don't modify any pre-existing data. In that same program, it reduces that to SOMETHING%.


If we want to go even further, *we can reduce our code down to zero `__check`s* if we code according to a "linear style", which will feel familiar to users of [Rust](https://www.rust-lang.org/), [Cone](https://cone.jondgoodwin.com/), [Austral](https://borretti.me/article/introducing-austral), or [Val](https://www.val-lang.dev/). Whether to go that far depends on the profiler, the area of the program, and the programmer.


This is Vale's strength: it's fast by default, and we can gradually optimize where it makes sense to.


# How do we put data in other objects?

The main benefit of single ownership is that it allows us to embed a struct inside another struct, or a struct inside an array. [# Technically, we don't need single ownership for this. Languages like C# allow us to put value types (such as `struct`s) directly inside classes and arrays. The real trick is whether we can take a reference to that embedded data.] How do we do that with generational references?


Recall our `__check` function from before, and notice the hardcoded `8`:

```
void __check(GenerationalReference genRef) {
  uint64_t currentGeneration = *(uint64_t*)((char*)genRef.allocation - 8);
  assert(genRef.rememberedGeneration == currentGeneration);
}
```


Instead of that `8`, we're going to subtract a tiny *offset-to-generation* number that we put in the top byte of the `genRef.allocation` pointer. [# Note that the offset-to-generation number can only fit in a single byte, which means that if a struct is more than 256 bytes wide, it will need another generation number inside it.]

*This will point us to the generation of the containing struct.*


For ARM CPUs, our `__check` function becomes this:

```
void __check(GenerationalReference genRef) {
  uint8_t offsetToGen = genRef.allocation >> 56;
  uint64_t currentGeneration = *(uint64_t*)((char*)genRef.allocation - offsetToGen);
  assert(genRef.rememberedGeneration == currentGeneration);
}
```

Our `__check` function just went from 4 instructions to 5 (or 6 on x86 [# For x86, we also need to add a bitwise-and to mask off those top 8 bits before we subtract. We don't do this on ARM because of its [top-byte ignore](https://www.kernel.org/doc/html/latest/arm64/memory-tagging-extension.html) feature.]). This isn't that much, especially considering we eliminate PERCENTAGE% of generation checks. For that low price, *we can now embed structs in other structs.*


This gives us more control over the layout of our data, which helps make our programs more cache-friendly.


# How do we put data on the stack?

This is where the scheme gets interesting, involving security, statistics, sorcery, and pragmatism. (Special thanks to [hc](https://lobste.rs/s/sglvcc/generational_references_2_3x_faster_than#c_ddjjg0) for this next key insight!)


Instead of remembering and incrementing an allocation's generation, we can just use a *pseudo-random number* instead.


Whenever we put a struct on the stack, we just conjure up a pseudo-random number and put it above the struct.

And whenever the struct disappears, we overwrite the generation with a zero, so that later `__check`s on it will fail.


This also has other benefits:

 * We can release memory back to the OS, since we don't need to remember a generation for a given slot of `free`d memory.
 * We can use the regular `malloc` instead of the `gmalloc` we invented above.
 * We don't have to check for overflow when a generation number hits 2^64.
 * Since generations no longer need to be at the top of a heap allocation, we can put one every 256 bytes to work better with the offset-to-generation number.


It solves a lot of problems, and is even a bit faster. [# When destroying an object, overwriting a generation with zero is faster than incrementing what was there before.]


The one theoretical downside is that one in every 18,446,744,073,709,551,616 `__check`s will be a false negative, a use-after-free that isn't detected.


Programming language theory is very black-and-white, so let's think about this from a security perspective instead.


*A generation is like a password.* A `__check` is like typing in the password and hitting enter. So how strong is this password?


On average, it will take [13,860,000,000,000,000,000](https://www.wolframalpha.com/input?i=.99999999999999999995%5E13860000000000000000) tries to trigger a false negative, which causes a use-after-free.


Let's put that into real terms!

 * If one observes a `__check` failure once a week, then it would take an average of [266 quadrillion cpu-years](https://www.wolframalpha.com/input?i=13860000000000000000+%2F+52) for that bug to cause a use-after-free.
 * If we observe a `__check` failure once a second, it would take [439 billion cpu-years](https://www.wolframalpha.com/input?i=13860000000000000000+%2F+%2860+*+60+*+24+*+365%29) on average. The bug will probably be fixed by then.
 * Even if we observe a `__check` failure a million times a second, it would take [439,498 cpu-years](https://www.wolframalpha.com/input?i=13860000000000000000+%2F+%281000000+*+60+*+60+*+24+*+365%29) on average.

(See LINK HERE for how we can know this!)


Keep in mind that before this happens, all these `__check`s have been failing *very loudly* [# They bring down the entire process (or containing thread) and write a lot of information to stderr.] You'll know about the problem long before you hit your 13 quadrillionth check. [# Or if you're lucky, the sun will have gone supernova by then, and you won't have to fix the bug.] [# This is a particularly nice approach because there's no stealth bugs lurking in `unsafe` blocks that we don't know about; any problem is detected very loudly and many times before it actually becomes a safety problem.]



# Hidden Benefits

Generational references and regions enable some pretty amazing new features:

 * [Seamless Concurrency](/blog/seamless-fearless-structured-concurrency): Since regions don't require any data coloring (like `Sync`/`Send`) or function coloring (like `async` or `?async`) we can use structured concurrency without refactoring the surrounding program.
 * [Perfect replayability](/blog/perfect-replayability-prototyped): Since generational references don't require nondeterministic collection, Vale was designed to replay a past run of the program to perfectly reproduce any error.
 * [Higher RAII](https://vale.dev/guide/structs#higher-raii): Vale's type system can use ownership to ensure that we don't forget to call a function at some point in the future, and its destructors can take in arguments and return values.


There are also a few more features that regions will enable, involving concurrency, transactions, and unique security benefits. [# One of the benefits of [sponsoring Vale](https://github.com/sponsors/ValeLang) is that you get access to all our future plans, and get to ask questions about them!] They're a little too early in the design phases to explain here, but stay tuned!


# Summary


Generational references and regions are a pretty promising combination with a lot of benefits:

 * Easy to use, especially for anyone coming from more complex languages like C++ or Rust.
 * They let us choose which parts of our code should be simple, and which should be optimized further.
 * They enable a lot of interesting new features, like [Seamless Concurrency](/blog/seamless-fearless-structured-concurrency), [Perfect replayability](/blog/perfect-replayability-prototyped), and [Higher RAII](https://vale.dev/guide/structs#higher-raii).


There are some downsides, as with any memory safety paradigm:

 * It occasionally adds an 8-byte generation number to the top of allocations.
 * Most pointers will only be 8 bytes, [# At any given time in a program, the vast majority of pointers are owning pointers, which require no generation.] but some will have an additional 8-byte generation with them.
 * There will be `__check`s at run-time, unless the programmer chooses to optimize them away.


But overall, the technique looks very promising, and feels pretty good to use.


<slice/>



# Notes


[#zerochecks]


<slice>
#zerochecks: Basic steps:

 1. Translate it to another single-ownership based language like Austral, Val, or Rust.
 1. Use indices instead of immutable references.
 1. Instead of taking in unique references, take in an owned value and return it.
 1. Translate back to Vale.

This works because any languages with single ownership (in the C++ and Vale sense of the word) can be used to achieve "zero-cost" memory safety (no RC, GC, or `__check`s, but still allowing bounds checking), and all programs in these languages can be translated to the others.

DRAFT: move this to a thoughts page.
</slice>


Try it out! In the Vale release, you can find our benchmarks [here]([BenchmarkRL](https://github.com/Verdagon/BenchmarkRL/tree/master/vale)). You can find the source code for the various approaches [here](https://github.com/ValeLang/Vale/tree/master/Backend/src/region) (feel free to swing by the [discord server](https://discord.gg/SNB8yGH) and we can point you to the right files).


*Note this caveat!* To isolate the difference between generational references and the other approaches, we allocate all non-primitive data on the heap. Future versions will add stack allocations. Once we address this limitations, we can get more precise benchmarks against the other approaches.
