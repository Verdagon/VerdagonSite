---
title: Regions Prototype
subtitle: Results and measurements!
author: Evan Ovadia
date: Jul 12, 2023
realm: blog
path: blog/regions-prototype
layout: annotated
namespace: c-blog m-annotated
sponsor: us
---


Three years, two states, and one pandemic ago, I wrote about a very weird idea: what if the type system could track which data existed before a pure call, to eliminate its memory safety overhead? [# See the [original article here](https://verdagon.dev/blog/zero-cost-refs-regions), but keep in mind that was written before I came up with generational references!]


And a couple months later, another weird idea struck: what if we use generational indices as the foundation for an entire language?


These ideas evolved in _weird_ ways. The first one evolved into a full [region-based borrowing system](https://verdagon.dev/blog/zero-cost-memory-safety-regions-overview). The second one become [generational references](/blog/generational-references). Together, they looked like they could form an *entirely new approach* to memory safety, one that doesn't use reference counting, tracing garbage collection, nor borrow checking.



The exciting part of all of this was that it's a completely opt-in system, and we're not restricted to a certain programming style. We could write our code in a *normal, comfortable way,* and then come back and add regions later for the parts of our program that we want to optimize. [# This is possible because it makes borrowing compose much better with shared mutability; you can do as much aliasing as you want and then turn the entire world immutable via `pure`.]


Even with only [basic use](/blog/zero-cost-memory-safety-regions-part-1-immutable-borrowing) of regions, we could reduce most generation checks [# A "generation check" happens whenever we dereference a generational reference. It checks to make sure the target object is still alive.] away with just a few annotations. Add in [isolates](/blog/zero-cost-memory-safety-regions-part-2-isolates) and linear style, [# Linear style is where we never make a reference to something unless handing it into a pure function.] and we can get generation checks *down to zero* for any Vale code.


If it all worked, then we could finally have a language where we could easily prototype, easily iterate, and then easily optimize. No language has ever nailed all three, so this was pretty exciting.


But alas, it was all a theory. We couldn't play with it, because it wasn't real yet!


# Building the Theory


Most of my free time for the past three years has gone into building out the compiler's foundations so that it could support this unexplored approach.


It was _hard_. Any kind of borrowing system is already pretty complex, but they also requires full generics, which are notoriously difficult. [# Previously, Vale had templates (like C++), not full generics.] [# It took the Golang team a decade to figure out their generics, and I don't blame them at all for that, after this struggle with generics!]


On top of that, an entire new compiler stage was needed to get regions and generational references to work seamlessly together. [# Under the hood, it reduces regions to "pure height" integers: negative for region generic parameters, zero for the default region, and increasing positive for every pure block. See Part 4 for more on this!]


Finally, a few months ago, *the regions prototype was finished*. It's rough around the edges, [# For example, its compile errors are very, _very_ verbose, and there are a lot of things that just trigger assertions in the compiler still. I'll be fixing all of these before merging it into the main branch.] but it successfully compiled something for the first time.


With that, I made the [first ever zero-check Vale program](https://github.com/Verdagon/RegionsBenchmarks/blob/main/cellular-automata/CellularAutomata.vale)! [# You can count how many generation checks in a program via the `--print_mem_overhead true` compiler flag.]

It was a program that uses [Cellular Automata](https://gamedevelopment.tutsplus.com/tutorials/generate-random-cave-levels-using-cellular-automata--gamedev-9664) to generate a level for a roguelike game.


<div style="position: relative; width: 128px; margin: auto">
<style>
.fade {
  animation-iteration-count: infinite;
  animation-timing-function: linear;
  animation-duration: 8s;
}
.fade-in-1 {
  opacity: 1;
  animation-name: fadeIn1Opacity;
}
.map-image {
  width: 128px;
  height: 128px;
  image-rendering: pixelated;
}
@keyframes fadeIn1Opacity {
  0%   { opacity: 1; }
  27%  { opacity: 1; }
  33%  { opacity: 0; }
  62%  { opacity: 0; }
  66%  { opacity: 0; }
  94%  { opacity: 0; }
  100%  { opacity: 1; }
}
.fade-in-2 {
  opacity: 0;
  animation-name: fadeIn2Opacity;
  position: absolute;
  top: 0;
  right: 0;
}
@keyframes fadeIn2Opacity {
  0%   { opacity: 0; }
  27%  { opacity: 0; }
  33%  { opacity: 1; }
  62%  { opacity: 1; }
  66%  { opacity: 0; }
  94%  { opacity: 0; }
  100%  { opacity: 0; }
}
.fade-in-3 {
  opacity: 0;
  animation-name: fadeIn3Opacity;
  position: absolute;
  top: 0;
  right: 0;
}
@keyframes fadeIn3Opacity {
  0%   { opacity: 0; }
  27%  { opacity: 0; }
  33%  { opacity: 0; }
  62%  { opacity: 0; }
  66%  { opacity: 1; }
  94%  { opacity: 1; }
  100%  { opacity: 0; }
}
</style>
<div class="fade fade-in-1">
<div style="text-align: left;"><b>0</b></div>
<img class="map-image" src="/images/cellular-automata-1.png"/>
</div>
<div class="fade fade-in-2">
<div style="text-align: center;"><b>1</b></div>
<img class="map-image" src="/images/cellular-automata-2.png"/>
</div>
<div class="fade fade-in-3">
<div style="text-align: right;"><b>2</b></div>
<img class="map-image" src="/images/cellular-automata-3.png"/>
</div>
</div>



Of course, it didn't work perfectly. Compilers are tricky. The slightest misstep will add extra instructions to the resulting assembly, causing artificial overhead in the final program. And sometimes, there's extra little bits of information you need to pass to the optimizer (or the CPU itself!) to trick it into the most optimal behavior.


To help me track down the problems, I kept comparing its assembly to the assembly generated by Vale's "unsafe" modes:

 * `unsafe_no_bounds` is similar to C; all memory-safety protections are turned off, and it only uses raw pointers for everything, rather than generational references.
 * `unsafe_with_bounds` then adds bounds checking for array accesses, similar to how Rust does it.


After a couple months of tracking down differences, the resulting assembly looked nearly identical to Vale's `unsafe_with_bounds` mode! Every difference was expected [# The only expected difference is that it put a pseudo-random generation number at the top of every allocation, though it never needed to read it for any generation checks. This is really just a montonically increasing register under the hood, to keep things fast. We'll be able to remove this once we add [isolates](/blog/zero-cost-memory-safety-regions-part-2-isolates) or `uni`que references.] and everything looked pretty reasonable.



Finally, I benchmarked the program again:


```
Summary
  './build_unsafe_no_bounds/main' ran
    1.18 ± 0.01 times faster than './build_unsafe_with_bounds/main'
    1.18 ± 0.01 times faster than './build_safe_fastest/main'
```



Success! Vale's normal mode (`safe_fastest` here) showed no slowdowns compared to only bounds checking.


In other words, *Vale's approach has no observable overhead.*


Finally seeing this was a shock, a relief, and almost surreal. No overhead! We knew it was possible in theory, but seeing it happen for real still felt very surprising.


Feel free to play with it! Just build from the [regions branch](https://github.com/Verdagon/Vale/tree/regions), check out the [benchmarking scripts](https://github.com/Verdagon/RegionsBenchmarks), and to ask any questions in the [discord server](https://discord.gg/SNB8yGH).


And before we get too excited, let's keep these important details in mind:

 * This is not benchmarking against languages like C and Rust directly. Those compilers have years of unrelated optimizations that would just confound the experiment, so I compare with `unsafe_no_bounds` and `unsafe_with_bounds` to isolate those variables and get a more accurate comparison of the memory safety approaches.
 * This was benchmarked on a Razer Blade 15" 2018 (512GB SSD) running Ubuntu 22.04, using [hyperfine](https://github.com/sharkdp/hyperfine) inside a [cset shield](https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html).
 * When I made larger programs, I observed quite a bit of optimizer noise, [# This is not the same thing as benchmark noise. This benchmarking setup reported very consistent run times (hence the `± 0.01` in the output).] where a minor change in one area would swing the entire benchmark results one way or another. [# In fact, when I switched the size of the generation numbers, it consistently had negative overhead (`1.13 ± 0.01`), which is a bit weird considering that there weren't that many generation numbers in the program anyway. It did change the register allocations, so I suspect that's dwarfing any performance differences from anything actually semantically different in the programs.] Benchmark results for the larger programs seemed rather fragile. We'll need a large set of benchmark programs to isolate away this optimizer noise.
 * In a larger program (a tiny roguelike game), I also observed that the optimizer didn't merge two identical branches of an if-statement, and missed a couple other obvious optimizations. I'm not sure how the presence of an integer (especially unread!) would affect this. It could even be a bug in LLVM, which are pretty common.


That last one hints that we might want our own Vale-specific pre-optimizer, similar to Rust's [Cranelift](https://github.com/bytecodealliance/wasmtime/tree/main/cranelift), since LLVM was designed more with C in mind.


Still, even with these details, these results are quite promising!



# What does this mean?

This means that generational references and regions combine to form a memory safety approach that is very, very fast.


It also means that this approach is actually viable, and could be desirable for quite a few software domains. A domain might desire this approach if:

 * It wants more predictable latency than tracing garbage collection.
 * It wants better performance and cache friendliness than reference counting.
 * It wants to prototype and iterate more easily than with borrow checking.


# Where does Vale go from here?

The above benchmarks compared Vale's safe mode to Vale's unsafe modes, for a more accurate comparison of the memory safety approaches.


However, there are still a few things to do before Vale can really go toe-to-toe with languages like C and C++.

 * I'll need to start a Vale-specific pre-optimizer, since LLVM's optimizer seems to have some problems reasoning about generations and immutability.
 * Vale still needs to support inline data, instead of putting all structs on the heap. (Note that that wouldn't affect the above benchmarks, which didn't use any structs.)
 * Regions are still just in the prototype phase. I'll need to smoothe out the rough edges, pay down a bit of tech debt, and merge this code in before doing anything else.


After this is merged in, I'll be making the standard library use regions so that every user will benefit from regions, even if their main program code doesn't use regions directly.


After that, I'll be polishing the rest of the compiler and making it more usable for a general audience.


# Conclusion

It's been an epic and exciting journey to get to this point! And now, we _finally_ have some measurements to show that zero-check programs are possible, and that they're as fast as we hoped.


I want to give a massive thanks to everyone that has helped with this endeavor, especially our contributors and sponsors! I definitely would not have made it to this point without your support.


If you're impressed with our [track record](https://vale.dev/roadmap#recent-additions) and believe in the [direction we're heading](https://vale.dev/roadmap), please consider [sponsoring us on GitHub](https://github.com/sponsors/ValeLang)!

<center>
  <a href="https://github.com/sponsors/ValeLang" class="donate-button">
     <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-heart icon-sponsor mr-1 color-fg-sponsors">
        <path fill-rule="evenodd" d="M4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.565 20.565 0 008 13.393a20.561 20.561 0 003.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.75.75 0 01-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5zM8 14.25l-.345.666-.002-.001-.006-.003-.018-.01a7.643 7.643 0 01-.31-.17 22.075 22.075 0 01-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.08 22.08 0 01-3.744 2.584l-.018.01-.006.003h-.002L8 14.25zm0 0l.345.666a.752.752 0 01-.69 0L8 14.25z"></path>
     </svg>
     Sponsor us on GitHub!
  </a>
</center>

With your support, we can bring regions to programmers worldwide!


See you next time,

- Evan Ovadia


<slice new-color="afterword"/>

# Vale's Vision

Vale aims to bring a new way of programming into the world that offers *speed*, *safety*, and *ease of use.*


*The world needs something like this!* Currently, most programming language work is in:

 * High-overhead languages involving reference counting and tracing garbage collection.
 * Complex languages (Ada/Spark, Coq, Rust, Haskell, etc.) which impose higher complexity burden and mental overhead on the programmer.

These are useful, but there is a *vast field of possibilities* in between, waiting to be explored!


Our aim is to explore that space, discover what it has to offer, and make *speed and safety easier than ever before.*


In this quest, we've discovered and implemented a lot of new techniques:

 * [Generational Memory](/blog/generational-references), for a language to ensure an object still exists at the time of dereferencing.
 * [Higher RAII](/blog/raii-next-steps), a form of linear typing that enables destructors with parameters and returns.
 * [Fearless FFI](/blog/fearless-ffi), which allows us to call into C without risk of accidentally corrupting Vale objects.
 * [Perfect Replayability](/blog/perfect-replayability-prototyped), to record all inputs and replay execution, and completely solve heisenbugs and race bugs.


These techniques have also opened up some new emergent possibilities, which we hope to implement:

 * [Region Borrow Checking](/blog/zero-cost-borrowing-regions-overview), which adds mutable aliasing support to a Rust-like borrow checker.
 * [Hybrid-Generational Memory](/blog/hybrid-generational-memory), which ensures that nobody destroys an object too early, for better optimizations.
 * [Seamless concurrency](https://verdagon.dev/blog/seamless-fearless-structured-concurrency), the ability to launch multiple threads that can access any pre-existing data without data races, without the need for refactoring the code or the data.
 * Object pools and bump-allocators that are memory-safe and decoupled, so no refactoring needed.


We also gain a lot of inspiration from other languages, and are finding new ways to combine their techniques:

 * We can mix an `unsafe` block with Fearless FFI to make a much safer systems programming language!
 * We can mix Erlang's isolation benefits with functional reactive programming to make much more resilient programs!
 * We can mix region borrow checking with Pony's `iso` to support shared mutability.

...plus a lot more interesting ideas to explore!


The Vale programming language is a novel combination of ideas from the research world and original innovations. Our goal is to publish our techniques, even the ones that couldn't fit in Vale, so that the world as a whole can benefit from our work here, not just those who use Vale.


Our medium-term goals:

 * Finish the Region Borrow Checker, to show the world that shared mutability can work with borrow checking!
 * Prototype Hybrid-Generational Memory in Vale, to see how fast and easy we can make single ownership.
 * Publish the Language Simplicity Manifesto, a collection of principles to keep programming languages' learning curves down.
 * Publish the Memory Safety Grimoire, a collection of "memory safety building blocks" that languages can potentially use to make new memory models, just like Vale combined generational references and scope tethering.


We aim to publish articles biweekly on all of these topics, and create and inspire the next generation of fast, safe, and easy programming languages.


If you want to support our work, please consider [sponsoring us on GitHub](https://github.com/sponsors/ValeLang)!

With enough sponsorship, we can:

 * Work on this full-time.
 * Turn the Vale Language Project into a 501(c)(3) non-profit organization.
 * Make Vale into a production-ready language, and push it into the mainstream!

<center>
  <a href="https://github.com/sponsors/ValeLang" class="donate-button">
     <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-heart icon-sponsor mr-1 color-fg-sponsors">
        <path fill-rule="evenodd" d="M4.25 2.5c-1.336 0-2.75 1.164-2.75 3 0 2.15 1.58 4.144 3.365 5.682A20.565 20.565 0 008 13.393a20.561 20.561 0 003.135-2.211C12.92 9.644 14.5 7.65 14.5 5.5c0-1.836-1.414-3-2.75-3-1.373 0-2.609.986-3.029 2.456a.75.75 0 01-1.442 0C6.859 3.486 5.623 2.5 4.25 2.5zM8 14.25l-.345.666-.002-.001-.006-.003-.018-.01a7.643 7.643 0 01-.31-.17 22.075 22.075 0 01-3.434-2.414C2.045 10.731 0 8.35 0 5.5 0 2.836 2.086 1 4.25 1 5.797 1 7.153 1.802 8 3.02 8.847 1.802 10.203 1 11.75 1 13.914 1 16 2.836 16 5.5c0 2.85-2.045 5.231-3.885 6.818a22.08 22.08 0 01-3.744 2.584l-.018.01-.006.003h-.002L8 14.25zm0 0l.345.666a.752.752 0 01-.69 0L8 14.25z"></path>
     </svg>
     Sponsor us on GitHub!
  </a>
</center>


