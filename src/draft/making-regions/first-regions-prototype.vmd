---
title: Vale's First Prototype for Immutable Region Borrowing
subtitle: Results and measurements!
author: Evan Ovadia
date: July 11, 2023
realm: blog
path: blog/regions-prototype
layout: annotated
namespace: c-blog m-annotated
sponsor: us
---


Three years, two states, and one pandemic ago, I wrote about a very weird idea: what if the type system could track which data existed before a pure call, to eliminate its memory safety overhead? [# See the [original article here](/blog/zero-cost-refs-regions), but keep in mind that was written before I came up with generational references!]


And a couple months later, another weird idea struck: what if we use generational indices as the foundation for an entire language?


These ideas evolved in _weird_ ways. The first one evolved into a full [region-based borrowing system](/blog/zero-cost-borrowing-regions-overview). The second one became [generational references](/blog/generational-references). Together, they looked like they could form an *entirely new approach* to memory safety, one that doesn't use reference counting, tracing garbage collection, nor borrow checking.


Basically, the coder writes their program in a normal C or C++ish way, and Vale's generational references keep everything memory safe. Then, the coder can use `pure` and [region borrowing](/blog/zero-cost-borrowing-regions-part-1-immutable-borrowing) to eliminate most generation check overhead. [# A "generation check" happens whenever we dereference a generational reference. It checks to make sure the target object is still alive.] Add in some [linear style](/blog/linear-types-borrowing), [# Linear style is where we never make a reference to something unless handing it into a pure function.] and we can get generation checks *down to zero* for any Vale code.


In other words, this could make our Vale code very, very fast.


The exciting part is that region borrowing is completely opt-in. We could write our code in a *normal, comfortable way,* and later add region borrowing for the parts that we want to optimize, almost like a more flexible, opt-in borrow checker. [# This is possible because it makes borrowing compose much better with shared mutability; you can do as much aliasing as you want and then turn the entire world immutable via `pure`.] We could choose which parts of our program should be as flexible as Java, or as fast as Rust, or anywhere in-between.


But alas, it was all a theory. We couldn't play with it, because it wasn't real yet!


<slice/>

# Building the Theory


Most of my free time for the past few years has gone into building out the compiler's foundations so that it could support this unexplored approach.


It was _hard_. Any kind of borrowing system is already pretty complex, but they also require full generics, which are notoriously difficult. [# Previously, Vale had templates (like C++), not full generics.] [# It took the Golang team a decade to figure out their generics, and I don't blame them at all for that, after this struggle with generics!]


On top of that, an entire new compiler stage was needed to get regions and generational references to work seamlessly together. [# Under the hood, it reduces regions to "pure height" integers: negative for region generic parameters, zero for the default region, and increasing positive for every pure block.]


Finally, a few months ago, *the regions prototype was finished*. It's rough around the edges, [# For example, its compile errors are very, _very_ verbose, and there are a lot of things that just trigger assertions in the compiler still. I'll be fixing all of these before merging it into the main branch.] but it successfully compiled something for the first time.


With that, I made the [first ever zero-check Vale program](https://github.com/Verdagon/RegionsBenchmarks/blob/main/cellular-automata/CellularAutomata.vale)! [# You can count how many generation checks in a program via the `--print_mem_overhead true` compiler flag.]

It was a program that uses [Cellular Automata](https://gamedevelopment.tutsplus.com/tutorials/generate-random-cave-levels-using-cellular-automata--gamedev-9664) to generate a level for a roguelike game.


<div style="position: relative; width: 128px; margin: auto">
<style>
.fade {
  animation-iteration-count: infinite;
  animation-timing-function: linear;
  animation-duration: 8s;
}
.fade-in-1 {
  opacity: 1;
  animation-name: fadeIn1Opacity;
}
.map-image {
  width: 128px;
  height: 128px;
  image-rendering: pixelated;
}
@keyframes fadeIn1Opacity {
  0%   { opacity: 1; }
  27%  { opacity: 1; }
  33%  { opacity: 0; }
  62%  { opacity: 0; }
  66%  { opacity: 0; }
  94%  { opacity: 0; }
  100%  { opacity: 1; }
}
.fade-in-2 {
  opacity: 0;
  animation-name: fadeIn2Opacity;
  position: absolute;
  top: 0;
  right: 0;
}
@keyframes fadeIn2Opacity {
  0%   { opacity: 0; }
  27%  { opacity: 0; }
  33%  { opacity: 1; }
  62%  { opacity: 1; }
  66%  { opacity: 0; }
  94%  { opacity: 0; }
  100%  { opacity: 0; }
}
.fade-in-3 {
  opacity: 0;
  animation-name: fadeIn3Opacity;
  position: absolute;
  top: 0;
  right: 0;
}
@keyframes fadeIn3Opacity {
  0%   { opacity: 0; }
  27%  { opacity: 0; }
  33%  { opacity: 0; }
  62%  { opacity: 0; }
  66%  { opacity: 1; }
  94%  { opacity: 1; }
  100%  { opacity: 0; }
}
</style>
<div class="fade fade-in-1">
<div style="text-align: left;"><b>0</b></div>
<img class="map-image" src="/images/cellular-automata-1.png"/>
</div>
<div class="fade fade-in-2">
<div style="text-align: center;"><b>1</b></div>
<img class="map-image" src="/images/cellular-automata-2.png"/>
</div>
<div class="fade fade-in-3">
<div style="text-align: right;"><b>2</b></div>
<img class="map-image" src="/images/cellular-automata-3.png"/>
</div>
</div>



Of course, it didn't work perfectly at first. Compilers are tricky. The slightest misstep in the compiler code will add extra instructions to the resulting assembly, causing artificial overhead in the final program. And sometimes, there's extra little bits of information you need to pass to the optimizer (or the CPU itself!) to trick it into the most optimal behavior.


To help me track down the problems, I kept comparing its assembly to the assembly generated by Vale's "unsafe" modes:

 * `unsafe_no_bounds` is similar to C; all memory-safety protections are turned off, and it only uses raw pointers for everything, rather than generational references.
 * `unsafe_with_bounds` then adds bounds checking for array accesses, similar to how Rust does it.


After a couple months of tracking down differences, the resulting assembly looked nearly identical to Vale's `unsafe_with_bounds` mode! Every difference was expected [# The only expected difference is that it put a pseudo-random generation number at the top of every allocation, though it never needed to read it for any generation checks. This is really just a monotonically increasing register under the hood, to keep things fast. We'll be able to remove this once we add [isolates](/blog/zero-cost-borrowing-regions-part-2-isolates) or `uni`que references.] and everything looked pretty reasonable.


<slice />

# The Benchmarks

Finally, I benchmarked the program again:


```
Summary
  './build_unsafe_no_bounds/main' ran
    1.18 ± 0.01 times faster than './build_unsafe_with_bounds/main'
    1.18 ± 0.01 times faster than './build_safe_fastest/main'
```



Success! Vale's normal mode (`safe_fastest` here) showed no slowdowns compared to only bounds checking.


In other words, *this approach has no observable overhead.* [# There could be overhead in theory, in the form of a nonatomic monotonically incrementing integer used for filling generations. It doesn't seem to affect the performance, likely because registers and simple arithmetic operations are so cheap on modern CPUs compared to the real bottleneck which is memory latency. The optimizer also often optimizes it out, since it sees nobody using these generations.]


Finally seeing this was a shock, a relief, and almost surreal. No overhead! We knew it was possible in theory, but seeing it happen for real still felt very surprising.


Feel free to play with it! Just build from the [regions branch](https://github.com/Verdagon/Vale/tree/regions), check out the [benchmarking scripts](https://github.com/Verdagon/RegionsBenchmarks), and ask any questions in the [discord server](https://discord.gg/SNB8yGH).


And before we get too excited, let's keep these important details in mind:

 * This is not benchmarking against languages like C and Rust directly. Those compilers have years of unrelated optimizations that would just confound the experiment, so I compare with `unsafe_no_bounds` and `unsafe_with_bounds` to isolate those variables and get a more accurate comparison of the memory safety approaches.
 * This was benchmarked on a Razer Blade 15" 2018 (512GB SSD) running Ubuntu 22.04, using [hyperfine](https://github.com/sharkdp/hyperfine) inside a [cset shield](https://manpages.ubuntu.com/manpages/trusty/man1/cset-shield.1.html).
 * When I made larger programs, I observed quite a bit of optimizer noise, [# This is not the same thing as benchmark noise. This benchmarking setup reported very consistent run times (hence the `± 0.01` in the output).] where a minor change in one area would swing the measurements one way or another. [# In fact, when I switched the size of the generation numbers, it consistently had negative overhead (`1.13 ± 0.01`), which is a bit weird considering that there weren't that many generation numbers in the program anyway. It did change the register allocations, so I suspect that's dwarfing any performance differences from anything actually semantically different in the programs.] Benchmark results for the larger programs seemed rather fragile. We'll need a large set of benchmark programs to isolate away this optimizer noise.
 * In a larger program (a tiny roguelike game), I also observed that the optimizer didn't merge two identical branches of an if-statement, and missed a couple other obvious optimizations. I'm not sure how the presence of an integer (especially unread!) would affect this. It could even be a bug in LLVM, which are pretty common.


That last one hints that we might want our own Vale-specific pre-optimizer, similar to Rust's [MIR](https://blog.rust-lang.org/2016/04/19/MIR.html), [# Thanks [bluejekyll](https://news.ycombinator.com/item?id=36690907) for this correction!] since LLVM was designed more with C in mind. [# We might want this anyway, as I'm pretty sure LLVM would treat generational references as undefined behavior, if it could figure out that we're intentionally accessing released memory.]


Still, even with these details, these results are quite promising!


<slice />

# What does this mean?

This means that generational references and regions combine to form a memory safety approach that is very, very fast.


It also means that this approach is actually viable, and could be desirable for quite a few software domains. A domain might desire this approach if:

 * It wants more predictable latency than tracing garbage collection.
 * It wants better performance and cache friendliness than reference counting.
 * It wants to prototype and iterate more easily than with borrow checking.


<slice />

# Where does Vale go from here?

The above benchmarks compared Vale's safe mode to Vale's unsafe modes, for a more accurate comparison of the memory safety approaches.


However, there are still a few things to do before Vale can really go toe-to-toe with languages like C and C++. [#easter]

 * I'll need to start a Vale-specific pre-optimizer, since LLVM's optimizer seems to have some problems reasoning about generations and immutability.
 * Vale still needs to support inline data, instead of the temporary solution of putting all structs on the heap. (Note that that wouldn't affect the above benchmarks, which didn't use any structs.)
 * Regions are still just in the prototype phase. I'll need to smoothe out the rough edges, pay down a bit of tech debt, and merge this code in before doing anything else.


After this is merged in, I'll be making the standard library use regions so that every user will benefit from them, even if their main program code doesn't use regions directly.


It's a pretty ambitious endeavor, so it's not clear how long this will take. With enough sponsorship I can work on this full-time, so if you believe in the direction we're heading, please consider sponsoring on [GitHub](https://github.com/sponsors/ValeLang) or [Patreon](https://www.patreon.com/user/about?u=5018675)!


<slice/>

# Conclusion

It's been an epic and exciting journey to get to this point! And now, we _finally_ have some measurements to show that zero-check programs are possible, and that they're as fast as we hoped.


I want to give a massive thanks to everyone that has helped with this endeavor, especially our contributors and sponsors! I definitely would not have made it to this point without your support.


Cheers!

- Evan Ovadia



<$include "/sponsoring.vmd"/>


<ignore>

Our job isn't done here yet. Even though we have zero-cost borrowing, there's another aspect of generational references we'd like to resolve: some objects still have (useless) generation numbers.


<<<<
For example, in this program, there's still a generation number above `Spaceship`, even though nobody ever checks it. [# It's pretty common for languages to have metadata for some objects. Swift's 8-byte reference counts, Java's 8-24 bytes ([metadata](https://www.javamex.com/tutorials/memory/object_memory_usage.shtml) plus a possible [remembered hash code](https://stackoverflow.com/a/3796963)). Vale's case is better because embedded structs don't have generations, only the containing local, heap allocation, or array slot. It's most similar to the 8-byte overhead we see in Rust's `SlotMap` or `GenerationalArena`.]

In the future, we'll be able to solve that by putting the Spaceship in its own private region called an isolate (link here) like `ship 'Spaceship = ...` which means that only one person can have a reference to it at a given time.
////
```
struct Spaceship {
  health i64;
  engine Engine;
  navigation Nav;
  wings Wings;
}
struct Engine { fuel i64; }
struct Nav { height i64; }
struct Wings { width i64; }

func main() {
  ship Spaceship = ...;
  println(ship.health);
}
```
>>>>

In the cellular automata program from before, each array had a generation at the top of it. The 1000x1000 `[][]bool` would have 1001 generations (one for the outer array, and one for each inner array). If that was an `[][]Spaceship`, it would have one generation per Spaceship. To avoid those generation numbers, we could have an array of isolated spaceships, like `[][]'Spaceship`.

</ignore>
