---
title: Borrow checking, RC, GC, and the Eleven (!) Other Memory Safety Approaches
subtitle: The Memory Safety Grimoire, Part 1
date: April 24, 2024
author: Evan Ovadia
realm: blog
path: grimoire/grimoire
layout: annotated
namespace: c-blog m-annotated
---


A fellow named Zeke came into my server one day.


*Zeke:* "Wait, so with generational references, we now have *four* ways to do memory safety?"

*Evan:* "In fact, there are *fourteen* by my count. Maybe more!" [# And I'd bet that someone on reddit or HN will comment on some I haven't heard before, and I'll have to change the title and add to the list!]

*Zeke:* "Fourteen?!"


I've gotten so used to it that it's not surprising to me anymore, so it's always a delight to vicariously feel people's surprise when I tell them this.


*Evan:* "Indeed," and I proceed to show him _the grimoire_ [#grimoire] that I've kept secret all these years.

*Zeke:* "How did you find all these?!"


At this point, I likely told him some nonsense like "I just kept my eyes open and collected them over the years!" but I think that you, my dear reader, deserve to know *the truth!* [# Or perhaps this entire article is just a clever ruse, the [mask behind the mask](https://hpmor.com/chapter/35), and the truth still remains a secret.]


This article is the introduction to my secret collection of memory safety techniques, which I call the *memory safety grimoire*.


With this wisdom, one can see the vast hidden landscape of memory safety, get a hint about where the programming world might head in the next decades, and even design new memory safety approaches. [# Such as [for C++](https://verdagon.dev/blog/vale-memory-safe-cpp)!]


! If you like this topic, check out this [Developer Voices episode](https://www.youtube.com/watch?v=UavYVf0UEoc) where Kris Jenkins and I talked about linear types and regions!


<slice>
#grimoire: A grimoire is a cursed spellbook, like the necronomicon.

However, those of weak wills should be careful not to read grimoires... they might end up pursuing the dark arts for years.
</slice>

# A Curséd Tome, and Ancient Memory Safety

(Or [skip to the list](#the-list)!)


Mid-2023, a team of archaeologists discovered [an ancient Mayan city in Campeche](https://www.reuters.com/world/americas/ancient-maya-city-discovered-mexican-jungle-2023-06-21/). I was on the ground as the team's consulting software engineer, as the Mayans were known as some of the earliest and most powerful practitioners of the software arts.


The hours are long and there's always the chance of being kidnapped and ransomed by the marauding bands of [white-nosed coatis](https://www.fodors.com/world/mexico-and-central-america/mexico/yucatan-and-campeche-states/experiences/news/photos/10-fantastic-beasts-of-the-yucatan-and-where-to-find-them), [# The coatis probably wouldn't be so endangered if they robbed banks instead, like [monkeys](https://zeenews.india.com/india/bizzare-monkey-steals-bag-with-rs-4-lakh-throws-currency-notes-outside-registry-office-in-up-2332462.html) and [snakes](https://globalnews.ca/news/9560059/calgary-poisonous-snake-bank-robbery-re-arrest/) do. Their 180-degree ankles would be invaluable for heisting.] but nobody asks me if I can add an RSS feed to a DBMS, so there's that. [# Free cookies to anyone who got [the reference](https://github.com/docker/cli/issues/267#issuecomment-695149477)!]


Our leader Ivan was hoping that this ancient city might contain a fifth memory safety tome, similar to the ancient borrow checking codex that Graydon Hoare found in the [Kukulkan pyramid](https://www.theguardian.com/world/2016/nov/17/mexican-pyramid-has-two-more-inside-scientists-discover) in 2016.


We made it to the central pyramid, and discovered a pedestal with a tattered tome, surprisingly intact after all this time. [# Our team lead [Ivan Spracj](https://inah.gob.mx/boletines/descubren-antigua-ciudad-maya-campeche-la-nombran-ocomtun-columna-de-piedra) explained that modern paper degrades much more quickly than the [amate](https://en.wikipedia.org/wiki/Amate) paper the Mayans used. Their ancient techniques were better than ours, both in paper and memory safety.] 


With my heart pounding, I approached the pedestal, looking closer at the inscriptions on the front. Sure enough, it had the Mayan symbols for "stack frame", "reference", and "region" on the front! We'd found it!


<slice />

# Blending Techniques


Before this, we only had three choices for memory safety, each with it's own tradeoffs:

 * Garbage collection [# By "garbage collection", we're referring to tracing garbage collection.] is easy, flexible, has high throughput, but uses [more energy](https://thenewstack.io/which-programming-languages-use-the-least-electricity/), [more memory](https://stackoverflow.com/a/764706/1424454), and has nondeterministic pauses.
 * Reference counting is simple and uses less memory, but is slow and can leak cycles. [# With good use of weak references, one can avoid the leaks.]
 * Borrow checking is faster and allows for aliasing inline data [# "Inline data" means we can have a struct's memory live on the stack, or inside another struct's memory, or directly in an array next to the other arrays' elements' memory. This is the default in C, and impossible in e.g. Javascript.], but can cause complexity and can't do patterns like [observers](https://en.wikipedia.org/wiki/Observer_pattern), [intrusive data structures](https://lwn.net/Articles/907876/), many kinds of RAII, etc. [# Luckily in Rust we can work around this limitation with reference counting!]


But we've long suspected that the Mayans had ways to blend these together at a more fundamental level.


We've certainly tried blending them before, and we've even had some success. For example, to work around the borrow checker in Rust we can put an object in an `Rc<RefCell<T>>` to make it reference counted, though that just delays the borrow checker to later when we `borrow`/`borrow_mut` the contents. Did the Mayans have a way to _truly blend_ the two approaches, like hinted on the walls of the Mayan [Sak Tz'i' tablets](https://www.livescience.com/maya-kingdom-discovered-in-mexico.html)? [# Below I talk about how we can use regions to blend borrowing and reference counting to get the benefits of both worlds.]


<ignore>
We've also learned that the Mayans had a way for RC and GC approaches to reference inline data (objects whose memory is inside another object's memory), [# This is referring to how we can use fat pointers or top-byte ignore to store the relative position of the object's header.] like we can do in C and Rust. Go [anonymous structs](https://blog.boot.dev/golang/anonymous-structs-golang/) and C#'s `struct`s come close, but we often can't make references to them: C#'s `ref`'s [restrictions](https://learn.microsoft.com/en-us/dotnet/csharp/language-reference/builtin-types/ref-struct) show how far we have to go.
</ignore>


Some ancient writings also describe a way to make fast languages that are not only safe but also _correct_ in a way that no languages are today [# Except for [Austral](https://borretti.me/article/introducing-austral)! It's safe because of borrow checking, and correct because it adds linear types. More on this below.], functional languages that use neither GC nor RC under the hood, [# This is referring to Kindelia's [HVM](https://github.com/HigherOrderCO/HVM) project!] and ways we can have unbounded reference counted objects _without a count integer_. [# See "Linear reference counting" below.] How did they do all this?


This is why we were so excited to find this tome. Perhaps it had the answers!


<slice />

# Impossibly Prophetic

As I deciphered the first few pages, I was shocked to find that it was referencing _things that hadn't happened yet_.


It was referencing to elucent's [Basil](https://degaz.io/blog/632020/post.html) language, Fernando's [Austral](https://borretti.me/article/introducing-austral), and Marco's [Forty2](http://forty2.is/) language... and yet carbon dating tells us that the book is hundreds of years old!


Somehow, the Mayans were looking _forward in time_ to techniques that were invented by people alive _today_. So maybe the Mayans were just time-traveling collectors, and this tome contains techniques from the recent past...

...and also the recent future. There's an entire half of this tome that seems to build on strange higher concepts that don't exist in our world yet. [# Almost as if their CPU designs were slightly different than our own, in a key way that unlocked more possibilities. I'm still scratching my head on this one.] Other pages seem to mention laws we haven't yet discovered. [# And there's one technique that I've tried to re-engineer thirty one (!) times without success: Hybrid-Generational Memory, one of my most elusive goals. I've gotten close by combining regions and generational references, but not in the automatic way that the Mayans seem to describe. Perhaps one of you can solve the rest of this puzzle!]


<slice />

# The List

To keep this post short, [# Because I've wasted all this space with the dramatic buildup, my bad!] I'll assume the reader knows the basics of reference counting, (tracing) garbage collection, and knows how borrow checking works at a high level.

Even so, this is a _very_ dense, compact overview of each technique, mainly meant as a starting point for further reading.

*Don't worry, I'll be posting many follow-up posts* ([RSS](https://verdagon.dev/rss.xml), [subreddit](https://reddit.com/r/vale)) *that describe each one much more clearly and how it fits into the larger puzzle.* [# I unfortunately can't give a timeline on this though, my health is a bit unstable lately.]

Afterward, I'll also talk about the interesting gaps in the puzzle, and the hints that might lead to discovery there.

Without further ado, here's the list!


<slice />


*1: Move-only programming* was the most surprising one to me. In this, every object can only be known to one variable (or field or array element), and that one "owner" can give it up to transfer it to another variable, field, array element, or function parameter or return. In Java terms, only one reference can point to an object at any given time.

Some of you may recognize these as [affine types](https://en.wikipedia.org/wiki/Substructural_type_system#Affine_types) (and also kind of [linear types](https://en.wikipedia.org/wiki/Substructural_type_system#Linear_types)), but will be surprised to learn that we can [write entire programs](https://verdagon.dev/blog/linear-types-borrowing) like this, without making any more references to any objects, as long as we're willing to go through some acrobatics. [# For example, you can't just point to something that is currently in a hash map... you first have to temporarily remove it so you can read it.]

Various languages build on this with different mechanisms: Rust adds [borrow checking](https://doc.rust-lang.org/rust-by-example/scope/borrow.html), Austral adds borrow checking and [linear typing](https://austral-lang.org/linear-types), and Vale has the [linear-aliasing model](https://vale.dev/linear-aliasing-model). I suspect the tome is hinting at other possible blends too. [# Specifically, if we can add Pony-style `val` to it, we might get an interesting result.]


<slice />


*2: Reference counting* is fairly mainstream. Some will be surprised to learn that it can coexist with tracing GC ([like in Python](https://devguide.python.org/internals/garbage-collector/index.html) and [Nim](https://nim-lang.org/blog/2020/10/15/introduction-to-arc-orc-in-nim.html)), and we also learned that there's a [whole spectrum](https://courses.cs.washington.edu/courses/cse590p/05au/p50-bacon.pdf) between the two.

And as it turns out, reference counting can be [blended with immutable region borrowing](https://github.com/Verdagon/RegionsRCCellularAutomataBenchmark/blob/main/README.md) to greatly reduce its cache misses and make it data-race safe, something no language has done yet. [# I couldn't resist prototyping this, so the Vale compiler actually has a flag that switches Vale from generational references to RC so we can see this in action. See [this experimental repo](https://github.com/Verdagon/RegionsRCCellularAutomataBenchmark/blob/main/README.md) for more.] [# [Nim](https://nim-lang.org/) could theoretically do this, but alas, I was unable to convince Araq that it was possible. I also thought for a while that Rust could do this, but it's unfortunately foiled by the `RefCell` escape hatch.]

For those who are into the more functional-programming side of things, I'm really interested in what Koka's doing with Perceus.
Also check out Koka's Perceus


<slice />


*3: Borrow checking* lets our code be as fast as C and even almost as safe as Haskell. [# I say almost because Rust's single-ownership nature sometimes introduces failure conditions that wouldn't exist in Haskell.] It works by ensuring that we only use pointers temporarily (in certain scopes) and in restricted ways to ensure others won't change the data that you're reading.

[Austral](https://borretti.me/article/introducing-austral) takes it a step further: it's not only safe, but also _correct_ by adding [liveness](https://en.wikipedia.org/wiki/Safety_and_liveness_properties) via linear types which any code can use to ensure that some future action will happen. This is a pattern I call [Higher RAII](https://verdagon.dev/blog/higher-raii-7drl) in Vale, but I think it naturally occurs in any language with linear types. [# Haskell can have linear types too! I foresee a future where functional programming and linear types are actually the best choice for safety-critical systems that can handle nondeterministic GC pauses. And the Mayans mention a new way that we can nearly eliminate those pauses...]

If you're curious for more, check out this [Developer Voices episode](https://www.youtube.com/watch?v=UavYVf0UEoc) where Kris Jenkins interviewed me on linear types and higher RAII.


<slice />


*4: Arena-only programming* is where we never use malloc or free, and [always use arenas instead](https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator), even for function returns. This is a familiar paradigm to users of C, Ada, Zig, and especially Odin which has a way to [automatically decouple code from allocator strategy](https://odin-lang.org/docs/overview/#implicit-context-system).

As described, this is more of a memory _management_ approach than a memory _safety_ approach. However, [Cyclone](https://en.wikipedia.org/wiki/Cyclone_(programming_language) and [Ada/SPARK](https://en.wikipedia.org/wiki/SPARK_(programming_language) show us that we can track which pointers are pointing into which arenas, to prevent any use-after-frees. [Verona](https://github.com/microsoft/verona) shows us that by combining arenas with regions (described below), we can take things even further. [# We could also combine arena-only programming with generational references, regions, constraint references, or MMM++.] 


<slice />


*5: Ada/SPARK* has a mechanism where a pointer cannot point to an object that is more deeply scoped than itself. If you imagine the stack growing to the right, [# "But Evan, stacks grow down!" [Listen here](https://memedrop.io/meme/ZRy5yVV68X7m), no it doesn't, nobody knows the orientation of the RAM chip in the computer.] pointers are only allowed to point left. If you really stretch your brain, this has some similarities to mutable value semantics or borrow checking. [# From [fghvbnvbnfe](https://news.ycombinator.com/item?id=14216128).]


<slice />


*6: Regions* are surprisingly flexible and powerful. I first learned about them from [Pony](https://www.ponylang.io/)'s [iso](https://tutorial.ponylang.io/reference-capabilities/guarantees.html#mutable-reference-capabilities) keyword: an `iso`'d object (and its contents) are only reachable from your pointer; nobody else points at that object or anything the object contains. In other words, it establishes an isolated subgraph of objects, and you hold the only reference to the entire thing.

[Colin Gordon](https://www.cs.drexel.edu/~csg63/papers/oopsla12.pdf) showed how we can "temporarily open" an isolated subgraph for a given scope, and afterward it would still be properly isolated.

I later wrote [an article](https://verdagon.dev/blog/zero-cost-refs-regions) about how we could temporarily open an isolated subgraph and see it as an *immutable region* so to speak [# "Explicit locking" in the linked article.] to _completely eliminate_ the memory safety cost for references pointing into that immutable region.

In that article I also explored how we can use a `pure` function to *temporarily reinterpret all pre-existing regions as immutable*, removing a vast amount of overhead. In 2023, we [completed the first prototype](https://verdagon.dev/blog/first-regions-prototype) showing this in action for generational references. It also [helps reference counting approaches](https://github.com/Verdagon/RegionsRCCellularAutomataBenchmark/tree/main) too.

Other languages such as [Forty2](https://forty2.is/) and [Verona](https://github.com/microsoft/verona) are going all-in on regions, and you'll see why further below.


! I'm thinking about separating "regions" into two separate simpler concepts in my writing: *regions* (a set of objects that can freely point to each other) and *region views* (a mutable or immutable view of a region). Region views are really what unlock regions' potential, I think. If anyone has opinions, [drop me an email](verdagon_epsa@verdagon.dev)!


<slice />


*7: Stack arenas* is an approach that's spiritually similar to arena-only programming, but it's automatic and does it for every stack frame. Elucent's [Basil](https://degaz.io/blog/632020/post.html) used to do this! It wasn't efficient, but the Mayans mention something about combining it with other techniques to make it a _lot_ faster. I can kind of see what they mean [# I _think_ it's compatible with reference counting, and I'm fairly certain it's compatible with linear types.] but I haven't seen anyone try it yet.


<slice />


*8: [Generational References](/blog/generational-references)* is a technique that prevents use-after-frees by telling us whether a pointer is pointing at a valid object, by comparing a pointer's accompanying "remembered" generation number to the "current" generation number living in the object it's pointing at. We increment an object's generation number whenever we want to destroy it, preventing any future accesses. This approach is made much faster by [regions](https://verdagon.dev/blog/zero-cost-borrowing-regions-overview): we never have to do that comparison if the object is in a temporarily immutable region, which we can establish with a `pure` function or block.

I hope other languages start using the generational references approach. There are a couple attempts in Rust ([here](https://github.com/Kile-Asmussen/genref) and [here](https://docs.rs/generational-box/latest/generational_box/)), but the language's rules prevent them from doing the faster variant described below.


<slice />


*9: Random Generational References* is a faster variant that lets us have "inline data", in other words it lets us put structs on the stack and inside arrays or other structs. This is similar to [memory tagging](https://source.android.com/docs/security/test/memory-safety/arm-mte), but much more reliable because of a wider tag (64 bits instead of 4) and when paired with perfect determinism, [# Perfect determinism is where the language doesn't introduce any features (e.g. reading uninitialized memory or casting pointers to integers) that could let nondeterminism leak into the program's logic. It's required for [perfect replayability](https://verdagon.dev/blog/perfect-replayability-prototyped)] more secure. [# Specifically, it means that we can defend against side-channel attacks at the program's architectural level, by never letting any nondeterminism leak into any untrusted code.]

This improvement is exciting to me because it lets the generation live right next to the object, and lets both live anywhere: on the stack, in an array, or inline inside another object. This makes it much faster in theory, because it means a program will incur less cache misses.

One can even blend this with [a technique that can reduce generation checks to zero](https://verdagon.dev/blog/linear-types-borrowing) where desired and regions for eliminating them everywhere else.

I think this blend has a lot of potential, because it has the strengths of C++ (architectural simplicity [# This means we can organize our program how we want, without interference from upwardly viral constraints (like async/await or borrow checking) or immutability concerns (like in functional programming). A litmus test for a language's architectural simplicity is whether you can implement a basic observer. Languages that don't have it will tend to have less stable APIs and a lot more refactoring.]) and Rust (memory safety) while being simpler and easier than both.

But I'm a bit biased of course, as any human would be about their own idea!


<slice />


*10: MMM++* [# I'm sure this has a better name, someone let me know!] is where objects are allocated from global arrays, and slots in those arrays are released and reused for other objects of the same type, thus avoiding use-after-free's normal memory unsafety problems. See [Arrrlang](https://verdagon.dev/blog/myth-zero-overhead-memory-safety) for a simple theoretical example, and one usually adds [other techniques too](https://verdagon.dev/blog/when-to-use-memory-safe-part-1#the-safer-way-to-use-mmm-languages) to make a real paradigm out of it. This is similar to how a lot of embedded, safety-critical, and real-time software works today, [# Including many servers, databases, and games. For example, [TigerBeetleDB](https://github.com/tigerbeetledb/tigerbeetle/blob/main/docs/TIGER_STYLE.md#safety) has a similar set of rules.] though no language comprehensively enforces it yet.


<slice />


*11: Tracing garbage collection* is familiar to all of us, but there's a surprising twist: there's a secret way to make a garbage collector without the stop-the-world pauses! [Pony](https://www.ponylang.io/) does this: by separating each actor into its own little world, each actor can do its own garbage collection without stopping the other ones. Its [ORCA](https://tutorial.ponylang.io/appendices/garbage-collection.html) mechanism then enables sharing data between the worlds via an interesting reference counting message passing mechanism.

[Verona](https://github.com/microsoft/verona) then takes this a step further by adding regions, giving the user more fine-grained control over when and where garbage collection might happen, and lets them use a regular bump allocator for a region instead if they wish.

If Verona or a new language allowed us to set the maximum memory for a GC'd region, that would make the entire approach _completely deterministic_, solving the biggest problem for garbage collection (in my opinion).

Don't tell anyone I said this, but I believe that 30 years from now, this blend is going to be the most widely used paradigm for servers.


<slice />


*12: Interaction nets* are a _very_ fast way to manage purely immutable data _without_ garbage collection or reference counting. The [HVM](https://github.com/HigherOrderCO/HVM) runtime implements this for Haskell. HVM starts with affine types (like move-only programming), but then adds an extremely efficient lazy `.clone()` primitive, so it can strategically clone objects instead of referencing them. Check out its [guide](https://github.com/HigherOrderCO/HVM/blob/master/guide/HOW.md) to learn more! [# And if someone has a better explanation, please send it to me! I don't understand interaction nets that well. I _think_ it's actually a blend of automatic borrowing and cloning, but the guide says it's not really borrowing, so I'm not sure.]


<slice />


*13: Constraint references* is a blend of reference counting and single ownership (in the C++ sense, unrelated to borrow checking). In this approach, every object has a single owner, doesn't necessarily need to be on the heap, and has a counter for all references to it. When we try to destroy the object, we just assert that there are no other references to this object.

This is used surprisingly often. Some [game developers](https://discord.com/channels/402956206529970177/402956206529970180/451828861861101569) have been using this for a long time, and it can be used as the memory safety model for an entire language like in [Gel](https://web.archive.org/web/20220111001720/https://researcher.watson.ibm.com/researcher/files/us-bacon/Dingle07Ownership.pdf). It supports a lot more patterns than borrow checking (intrusive data structures, graphs, observers, back-references, dependency references, callbacks, delegates, many forms of RAII, etc).

However, this checking is at run-time. Halting in release mode is often undesirable, so this technique shines the most when it's very targeted or when we can fall back to a different strategy in release mode.


<slice />


*14: Linear reference counting* is an elusive concept, where we can completely eliminate the counter integer, and do all of the reference counting at compile time. No language can do this today, but there might be a way to get close with linear types. [# Matthieu's [static-rc](https://github.com/matthieu-m/static-rc) crate gets pretty close to this, but without linear types in Rust, it has to leak under certain conditions. It's quite possible that his work on static-rc inspired this idea!] Basically, we have two types of linear reference:

 * A "tine" reference remembers (in its type, at compile-time) L, the number of forks to get from the original value to here.
 * A "fork" reference holds the original value (or a tine reference [# This is another difference between this idea and static-rc crate, I believe this will allow us to "borrow the borrow references" in objects, without lifetimes, rather than just on the stack. (Update: This might not be the case, see [this thread](https://www.reddit.com/r/rust/comments/1d06gjo/comment/l5x11po/) for a more accurate comparison)]), and remembers L and also N, the number of L+1 tine references created at the same time as this fork reference. Reclaiming the contents requires destroying this and all (N) of the L+1 tine references.

I don't expect anyone to understand that rushed explanation, but I hope to write an article soon on this! [# I also talked about it a bit on discord [here](https://discord.com/channels/398263331808346123/734119020613075052/1135196844360736798) and [here](https://discord.com/channels/398263331808346123/734119020613075052/1137411293310099546) and [here](https://discord.com/channels/398263331808346123/734819934760075264/1141854334171222098) and on Reddit [here](https://www.reddit.com/r/rust/comments/1d06gjo/comment/l5x11po/). (If someone knows how to make better publicly-accessible discord logs, let me know!)]

! I'm not actually sure what kind of architectural restrictions it might impose, how situational it is, or if it even works at all. It's just something I came up with--I mean uh, _the grimoire_ mentions--as a hypothetical avenue to explore.


<slice />


*15: Not-MVS* is a very interesting approach. Imagine a Java or Swift where every object has exactly one reference pointing to it at any given time (similar to move-only programming) but that reference can be lent out to a function call. It's like a Rust with no shared references (`&`), only unique references (`&mut`) which can't be stored in structs. It's simple, fast, and powerful, though we may have to `.clone()` more often than even Rust programs.

Edit: I originally thought this is how Mutable Value Semantics worked, and I was totally wrong ([thanks to dist1ll for the correction](https://lobste.rs/s/mutdyp/borrow_checking_rc_gc_eleven_other_memory#c_tczfin)!). I'll leave it here as "Not-MVS" because I think what I described would _probably_ still work as a memory safety approach.

For those curious about Mutable Value Semantics, dist1ll writes:

> MVS as implemented by Hylo has multiple parameter passing modes. The immutable mode (which is equivalent to Rust’s &) is the default, but you can declare a mutable mode (i.e. &mut) with the inout keyword. The other two parameter passing modes are for transferring ownership (sink) and callee initialization (set).


<slice/>

*16: [CHERI](https://www.cl.cam.ac.uk/research/security/ctsrd/cheri/)* is a hardware-software blend that can run languages like C in a memory-safe way. In CHERI, a pointer is represented as a 128-bit [# Or 64-bit on 32-bit systems.] type that contains an address range and permissions describing the operations that may be done with the pointer (and some other things), to achieve spatial memory safety. The hardware keeps track of whether something is a pointer or not via a 1-bit tag.

! Check out [David Chisnall's comment](https://lobste.rs/s/mutdyp/borrow_checking_rc_gc_eleven_other_memory#c_x67f1n) for some good explanation and clarifications!

[Cornucopia](https://ieeexplore.ieee.org/document/9152640) adds temporal memory safety to that with special allocators that don't reuse memory in a page until it's empty and all of the existing capabilities have been revoked [# This is an important memory safety concept: Memory unsafety comes not from use-after-free, but _use-after-reuse_. In fact, even that's too loose; memory unsafety comes from "use after shape change", which I'll explain later in the grimoire.] via an application-wide memory sweep done concurrently in the background. [Cornucopia Reloaded](https://www.ietfng.org/nwf/publications-and-talks/2024-cornucopia-reloaded.html) and [CHERIoT](https://cheriot.org/papers/2023-micro-cheriot-uarch.pdf) are also mechanisms that bring use-after-free protections to CHERI.

If a new language used a system like this plus some techniques to prevent use-after-free on the stack, it could have a brand new memory safety model nobody's seen before.


<slice />


*17: Neverfree* doesn't really count, but I'll mention it as a bonus item just for fun. Basically, just don't call `free`! If you never `free` memory, you can't use-after-free, which instantly solves the hardest part of memory safety. [# One might need to also add some bounds checking and a few other measures, but it's a start!] The idea is from this famous [email conversation](https://devblogs.microsoft.com/oldnewthing/20180228-00/?p=98125):

Norman Cohen said:

> The only programs I know of with deliberate memory leaks are those whose executions are short enough, and whose target machines have enough virtual memory space, that running out of memory is not a concern. (This class of programs includes many student programming exercises and some simple applets and utilities; it includes few if any embedded or safety-critical programs.)

Kent Mitchell replied:

> I was once working with a customer who was producing on-board software for a missile. In my analysis of the code, I pointed out that they had a number of problems with storage leaks. Imagine my surprise when the customers chief software engineer said "Of course it leaks". He went on to point out that they had calculated the amount of memory the application would leak in the total possible flight time for the missile and then doubled that number. They added this much additional memory to the hardware to "support" the leaks. Since the missile will explode when it hits its target or at the end of its flight, the ultimate in garbage collection is performed without programmer intervention.

It kind of makes sense in a way. If you have a program that uses all the memory all the way until the end (like `sort`), why not skip the expensive `free`s and let the OS clean it up when the process exits? You can't use-after-free if you never `free`!



! Wait a minute, this list goes to 17, yet the intro only mentions 14! I actually did that because a couple might overlap [#overlaps] and a couple of them are half-approaches [#needcombine], and that last one is just here for fun. Besides, as I learn more approaches and add them to the list, the title will get more and more out of date anyway.


<slice>
#overlaps: Ada/SPARK might be a blend of MMM++ and arena-only programming, perhaps. I haven't used Ada/SPARK, so let me know!

#needcombine: It could be said that regions on its own isn't really a memory safety approach, and it could be said that arena-only programming is just a memory _management_ technique. But hey, when you put those two halves together you get [Verona](https://github.com/microsoft/verona)'s memory safety approach, so together they probably count as one.

</slice>

# What do we do with this avalanche of knowledge?

Perhaps someone, after reading this article, will go forth and *design a new memory safety blend!* It's not impossible, I even used this grimoire to [make a theoretical blend for C++](https://verdagon.dev/blog/vale-memory-safe-cpp).


The world needs more memory safety blends and techniques! Especially ones that let us have better architectures, more simplicity, and less constraints. And who knows, searching for new techniques and blends might lead to interesting spinoff features, like Vale's [perfect replayability](/blog/perfect-replayability-prototyped) and [concurrency without data-coloring](/blog/seamless-fearless-structured-concurrency).


We often fall into a mental trap where we optimistically believe that we've solved everything there is to solve, and pessimistically believe there's nothing left to discover. That mental trap is a mind-killer, because we can't discover new things if we aren't open to their existence.

In fact, one of my favorite cognitive science tricks is to convince myself that there is a better solution and it's _just barely_ within reach if I just give it a little more thought. For some reason, that removes the mental barriers and lets one truly, fully explore. [# This is a great technique in algorithm interviews, by the way.]


If someone were to ask me why we should keep looking, I'd show them the unique strengths of each paradigm:

 * RC's weak pointers let us easily know [# In other languages, we need some sort of central tracking data structure to pull this off.] when another object's logical lifetime has ended, which is a surprisingly common need when you're looking out for it.
 * C and C++ let us use intrusive data structures, like no other language can. [# GC'd languages generally don't allow long-lived references to inline data, and Rust's borrow checker [prevents intrusive data structures](https://lwn.net/Articles/907876/)]
 * Austral and Vale's linear types allow for [Higher RAII](https://verdagon.dev/blog/higher-raii-7drl), which lets compilers prevent a lot of logic problems.
 * GC is the easiest, and depending on one's definitions, the safest too. [#gcsafe]

...and then I'd show them the whole list of approaches, and how many ways there are to blend them together.


With that in mind, it's pretty clear that memory safety is truly a wide-open world, waiting to be explored!


<slice>
#gcsafe: It's a tricky topic. When one thinks not just about memory safety but about safety in general, a null-safe functional GC'd language has an edge over other approaches, even over borrow checking which forces long-term-referrable objects into central collections which have their own potential edge cases.
</slice>

# More in the Grimoire

The above list is not complete, of course. There are some half-deciphered hints and building blocks in the grimoire that might be able to assist memory safety models in new ways.


! Beware: We don't know which of these techniques actually help memory safety, and which summon ancient demons. Proceed at your own risk!


Here's just a handful:

 * *[Type stability](https://engineering.backtrace.io/2021-08-04-slitter-a-slab-allocator-that-trusts-but-verifies/)* shows us that use-after-free isn't the enemy, but rather a simplistic approximation of the enemy. The real memory safety problems arise when we access some memory after we've released and _reused it for something of a different type._ [# "Shape stability" takes that a step further: we can reuse e.g. an integer's memory for a float and still not trigger memory unsafety, so really the problem is when we confuse a pointer for an integer or vice versa.]
 * *Final references* (like in Java) can help a lot in designing memory safety models. I won't explain too much here, but [email](verdagon_epsa@verdagon.dev) or discord me (Verdagon) and I can explain there. I dare not write publicly about what these unlock for memory safety, for what it would do to the world.
 * *Unique References*, in other words, guaranteeing that you have the only usable reference to an object, has been the key breakthrough in more approaches than I can count, including borrow checking, mutable value semantics, move-only programming, etc. There are even techniques that can make any object temporarily unique (like [how Swift's inout works](https://news.ycombinator.com/item?id=15294334), or how in generational references we can just temporarily change the generation). [#easter]
 * *Change detectors* is a mechanism that will track at run-time whether something's been changed. Java collections [use a modCount](https://stackoverflow.com/a/34629665/1424454) to prevent modifying while iterating, and one could conceivably use this to assist in memory safety as well.
 * *Check-on-set* is a pattern where we check at run-time if we're allowed to modify an object in a certain way. The best example of this how we can [freeze](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/freeze) a Javascript object, and whenever we modify an object, the runtime will assert it's not frozen. Anything that can guarantee an object immutable could be used for a memory safety approach.
 * *Thread isolation* is where we guarantee that an object is only visible to one thread at a given time. This property has helped enable borrow checking, generational references, Vale's immutable region borrowing, and faster forms of reference counting. It's likely important to other potential memory safety approaches.
 * *Page Headers* are where an allocator can strategically put metadata about an object at the top of its 4096-byte page.
 * *Fat pointers* is where some other data always accompanies a pointer. The biggest example of this is Rust's [trait references](https://stackoverflow.com/questions/57754901/what-is-a-fat-pointer) and Vale uses it for its [generational references](/blog/generational-references) memory safety approach.
 * *[Top-byte ignore](https://www.linaro.org/blog/top-byte-ignore-for-fun-and-memory-savings/)* refers to how some CPUs ignore the top byte of any particular pointer, so you can conveniently put anything you want there. You can even simulate top-byte-ignore on other systems by manually masking that byte off before dereferencing. This can be used to e.g. store how far the reference count integer is, so that you can point to the interior of a reference counted object. There are more arcane ways to use bits in the middle and end too. [# We can also use the lower bits if we know the alignment of the data we're pointing to. We might even be able to use the bits in the middle by manually specifying the address `mmap` should give us.]
 * And many more, hopefully in upcoming articles!


You would be surprised how many little tricks can be used to complete or assist new memory safety models.


If you know of any more memory safety techniques, or want to see in-progress decipherings, then come on over to the #grimoire channel in the [Vale discord](https://discord.com/invite/SNB8yGH).



<slice>
#easter: Easter egg note!

[William "Billy" Windsor I](https://en.wikipedia.org/wiki/William_Windsor_%28goat%29) is a [cashmere goat](https://en.wikipedia.org/wiki/Cashmere_goat) who served as lance corporal in the British Army's [Royal Welsh](https://en.wikipedia.org/wiki/Royal_Welsh) 1st Battalion from 2001 until 2009.

He was demoted to rank [fusilier](https://en.wikipedia.org/wiki/Fusilier) for three months for inappropriate behaviour during the 2006 [Queen's Official Birthday](https://en.wikipedia.org/wiki/Queen%27s_Official_Birthday) celebrations while on active duty with the battalion on Cyprus.

If you read this note, mention "that one lance corporal goat" anywhere on HN or reddit! [Nobody will believe you](https://youtu.be/-UBgNREvlIo).

(Cheers to [cbsmith](https://news.ycombinator.com/item?id=36691658), [kubanczyk](https://news.ycombinator.com/item?id=36692089), [TheGoldenMinion](https://www.reddit.com/r/programming/comments/14wu830/comment/jrklcry/?utm_source=reddit&utm_medium=web2x&context=3), [lovich](https://news.ycombinator.com/item?id=36691564), [padraig_oh](https://www.reddit.com/r/programming/comments/14wu830/comment/jrkmjiw/?utm_source=reddit&utm_medium=web2x&context=3), and [leksak](https://news.ycombinator.com/item?id=36691737) for the [last one](/blog/easter-egg-notes)!)
</slice>


# That's all!

I hope you enjoyed this article! It represents my findings after a decade of searching and designing, so I hope it helps a lot of people out there.


In the next post, I'll talk about how we can blend reference counting with some of the above techniques to drastically reduce their overhead and add fearless concurrency, so keep an eye out on our [RSS feed](https://verdagon.dev/rss.xml), [twitter](https://twitter.com/vale_pl), [discord server](https://discord.gg/SNB8yGH), or [subreddit](https://reddit.com/r/vale)!


Donations and sponsorships for Vale are currently paused, but if you like these articles, please [Donate to Kākāpō Recovery](https://www.doc.govt.nz/kakapo-donate) and let me know. I love those birds, let's save them!


Cheers,

- Evan Ovadia

<slice new-color="afterword"/>

# Thank you!

I want to give a huge thanks to [Arthur Weagel](https://github.com/aweagel), [Kiril Mihaylov](https://github.com/KirilMihaylov), [Radek Miček](https://github.com/radekm), [Geomitron](https://github.com/Geomitron), [Chiuzon](https://github.com/chiuzon), [Felix Scholz](https://github.com/soupertonic), [Joseph Jaoudi](https://github.com/linkmonitor), [Luke Puchner-Hardman](https://github.com/lupuchard), [Jonathan Zielinski](https://github.com/tootoobeepbeep), [Albin Kocheril Chacko](https://github.com/albinkc), [Enrico Zschemisch](https://github.com/ezschemi), [Svintooo](https://github.com/Svintooo), [Tim Stack](https://github.com/tstack), [Alon Zakai](https://github.com/kripken), [Alec Newman](https://github.com/rovaughn), [Sergey Davidoff](https://github.com/Shnatsel), [Ian (linuxy)](https://github.com/linuxy), [Ivo Balbaert](https://github.com/Ivo-Balbaert/), [Pierre Curto](https://github.com/pierrec), [Love Jesus](https://github.com/loveJesus), [J. Ryan Stinnett](https://github.com/jryans), [Cristian Dinu](https://github.com/cdinu), and [Florian Plattner](https://github.com/lasernoises) (plus a very generous anonymous donor!) for sponsoring Vale over all these years.

Recent events may have forced me to stop coding Vale for a while and led me to pause donations and sponsorships, but your support all this time is still giving me spirit and strength! Things are looking up, and I hope to be back soon.
